[
["index.html", "CM 2110 Calculus and Statistical Distributions Course Syllabus Pre-requisites Learning Outcomes Outline Syllabus Method of Assessment Recommended Texts Lecturer Schedule", " CM 2110 Calculus and Statistical Distributions Dr. Priyanga D. Talagala 2020-05-29 Course Syllabus Pre-requisites CM 1110 Remark: This course module contains two main sections: (1) mathematics and (2) statistics. This syllabus is designed for the statistics section. Lectures for mathematics section and statistics section are conducted by two lecturers as two separate sub modules (1.5 hour lectures/Week). End Semester Examination is conducted as a single examination. Learning Outcomes On successful completion of this module, students will be able to plan more carefully the design of experiment in advance which provide evidence for or against theories of cause and effect and make inferences about population characteristics based on sample information and thereby solve data analysis problems in different application domains. (R(https://cran.r-project.org/) and RStudio are also freely available to install on your own computer). Get the Open Source Edition of RStudio Desktop. RStudio allows you to run R in a more user-friendly environment. Outline Syllabus Functions of Several Variables Linear Algebra Coordinate Systems &amp; Vectors Differential Equations Statistical Distributions Estimation Hypothesis Testing Design of Experiments Method of Assessment Mid-semester examination End-semester examination Recommended Texts Casella, G., &amp; Berger, R. L. (2002). Statistical inference (Vol. 2, pp. 337-472). Pacific Grove, CA: Duxbury. Mood, A.M., Graybill, F.A. and Boes, D.C. (2007): Introduction to the Theory of Statistics, 3rd Edn. (Reprint). Tata McGraw-Hill Pub. Co. Ltd. Montgomery, D. C. (2017). Design and analysis of experiments. John wiley &amp; sons. Lecturer Dr. Priyanga D. Talagala Schedule Lectures: Friday [9.15 am - 10.45 am] Tutorial: Friday [11.00 am - 12.30 pm] Consultation time: Friday [8.15 am to 9.00 am] "],
["statistical-distributions.html", "Chapter 1 Statistical Distributions Recap: CM 1110-Probability 1.1 Random Variable 1.2 Probability Mass Function 1.3 Probability Density Function 1.4 Cumulative Distribution Function 1.5 Descriptive properties of distributions 1.6 Models for discrete distributions 1.7 Models for continuous distributions", " Chapter 1 Statistical Distributions Recap: CM 1110-Probability Axioms of probability Probability of an event quantifies the uncertainty, randomness, or the possibility of occurrence the event. The probability of event E is usually denoted by \\(P(E)\\). Mathematically, the function \\(P(.)\\) is a set function defined from sample space \\((\\Omega)\\) to \\([0, 1]\\) interval, satisfying the following properties. These are called the ‘axioms of probability’. Axiom 1: For any event \\(A\\), \\(P(A) \\geq 0\\) Axiom 2: \\(P(\\Omega) = 1\\) Axiom 3: If \\(A_1, A_2, \\dots, A_k\\) is a finite collection of mutually exclusive events, then \\[P(A_1\\cup A_2\\cup \\dots \\cup A_k)= \\sum_{i=1}^kP(A_i)\\] If \\(A_1, A_2, \\dots\\) is an infinite collection of mutually exclusive events, then \\[P(A_1\\cup A_2\\cup \\dots)= \\sum_{i=1}^\\infty P(A_i)\\] NOTE Axioms 1 and 2 imply that for any event \\(E\\), \\(0 \\leq P (E) \\leq 1\\). \\(P (E) = 1 \\iff\\) the event E is certain to occur. \\(P (E) = 1 \\iff\\) the event E cannot occur. Methods for determining Probability There are several ways for determining the probability of events. Usually we use the following methods to obtain the probability of events. Classical method Relative frequency method (Empirical approach) Subjective method Using probability models 1.1 Random Variable Some sample spaces contain quantitative (numerical) outcomes, others contain qualitative outcomes. Often it is convenient to work with sample spaces containing numerical outcomes. A function that maps the original sample space into the real numbers is called a ‘random variable’. This is more useful when the original sample space contains qualitative outcomes. Definition 1: Random Variable Let \\(\\Omega\\) be a sample space. Let \\(X\\) be a function from \\(\\Omega\\) to \\(\\Re\\) (i.e. \\(X:\\Omega \\rightarrow \\Re\\)). Then \\(X\\) is called a random variable. A random variable assigns a real number to each outcome of a sample space. In other words, to each outcome of an experiment or a sample point \\(\\omega_i\\), of the sample spaces, there is a unique real number \\(x_i\\), known as the value of the random variable \\(X\\). The range of the random variable is called the induced sample space. A note on notation: Random variables will always denoted with uppercase letters and the realized values of the random variable (or its range) will be denoted by the corresponding lowercase letters. Thus, the random variable \\(X\\) can take the value \\(x\\). Each outcome of a sample space occurs with a certain probability. Therefore, each possible value of a random variable is associated with a probability. Any events of a sample space can be written in terms of a suitably defined random variable. 1.1.1 Types of Random Variables A random variable is of two types Discrete Random Variable Continuous Random Variable 1.1.1.1 Discrete Random Variable If the induced sample space is discrete, then the random variable is called a discrete random variable. Example 01 Consider the experiment of tossing a coin. Express the following events using a suitably defined random variable \\(H=\\) The event of getting a head \\(T=\\) The event of getting a tail Example 02 Consider the experiment of rolling of a die. Express the following events using a suitably defined random variable \\(A=\\) The event that the number faced up is less than 5 \\(B=\\) The event that the number faced up is even \\(C=\\) The event that the number faced up is is 2 or 5 Example 03 Consider the experiment of tossing a coin 10 times. Then the sample space \\(\\Omega\\) contains \\(2^{10} = 1024\\) outcomes. Each outcome is a sequence of 10 H’s and T’s. Express the following events in terms of a suitably defined random variable. \\(D=\\) The event that the number of heads is 5 \\(E=\\) The event that the number of tails is less than 4 1.1.1.2 Continuous Random Variable If the induced sample space is continuous, then the random variable is called a continuous random variable. Example 04 Consider the experiment of measuring the lifetime (in hours) of a randomly selected bulb. Express the following events in terms of a suitably defined random variable. \\(F=\\) The event that the lifetime is less than 300 hours \\(G=\\) The event that the lifetime is 1000 hours 1.2 Probability Mass Function Definition 2: Discrete density function of a discrete random variable If \\(X\\) is a discrete random variable with distinct values \\(x_1, x_2, \\dots, x_n, \\dots,\\) then the function, denoted by \\(f_X(.)\\) and defined by \\[\\begin{equation} f_X(x) = \\begin{cases} P(X=x) &amp; \\text{if } x=x_j, j=1,2,\\dots,n,\\dots\\\\ 0 &amp; \\text{if } x \\neq x_j \\end{cases} \\end{equation}\\] is defined to be the discrete density function of \\(X\\). The values of a discrete random variable are often called mass points. \\(f_X(x)\\) denotes the mass associated with the mass point \\(x_j\\). Probability mass function discrete frequency function and probability function are other terms used in place of discrete density function Probability function gives the measure of probability for different values of \\(X\\). 1.2.1 Properties of a Probability Mass Function Let \\(X\\) be a discrete random variable with probability mass function \\(f_X(x)\\). Then, For any \\(x\\in \\Re\\), \\(0\\leq f_X(x) \\leq 1.\\) Let \\(E\\) be an event and \\(I= \\{X(\\omega):\\omega \\in E\\}.\\) Then \\(P(E) = P(X\\in I) = \\sum_{x \\in I}f_X(x).\\) Let \\(R = \\{X(\\omega):\\omega \\in \\Omega\\}.\\) Then \\(\\sum_{x\\in \\Re} f_X(x) = 1.\\) 1.2.2 Representations of Probability Mass Functions Example 05 Consider the experiment of tossing a fair coin. Let \\[\\begin{equation} X = \\begin{cases} 0 &amp; \\text{if the outcome is a Tail }\\\\ 0 &amp; \\text{if the outcome is a Head} \\end{cases} \\end{equation}\\] Find the probability mass function of \\(X\\). Is \\(X\\) discrete or continuous? 1.2.2.1 Using a table 1.2.2.2 Using a function 1.2.2.3 Using a graph 1.3 Probability Density Function Let \\(X\\) be a continuous random variable. Then, it is not possible to define a pmf \\(f_x\\) with properties mentioned in Section . Why? Instead, we can find a function \\(f_x\\) with the some different properties. Probability density function (pdf) of a continuous random variable is a function that describes the relative likelihood for this random variable to occur at a given point. 1.3.1 Properties of a Probability Density Function Let \\(X\\) be a continuous random variable with probability density function \\(f_x\\). Then, For any \\(x\\in \\Re\\), \\(f_X(x) \\geq0\\). Let \\(E\\) be an event and \\(I= \\{X(\\omega):\\omega \\in E\\}.\\) Then \\(P(E) = P(X\\in I) = \\int_If_X(x)dx.\\) Let \\(R = \\{X(\\omega):\\omega \\in \\Omega\\}.\\) Then \\(\\int_\\Re f_X(x)dx= 1.\\) 1.3.2 Existence of pdf To see the existence of such a function, consider a continuous random variable \\(X\\), Suppose that we have a very large number of observations, \\(N\\), of \\(X\\), measured to high accuracy (large number of decimal places). consider the following grouped frequency table and the histogram constructed from those data. The height of the bar on a class interval of this histogram is equal to the relative frequency per unit in that class interval. Interval Class boundaries Class frequency Height of the bar Area of the bar \\(I_1\\) \\(x_1 -\\delta x/2, x_1+\\delta x/2\\) \\(n_1\\) \\(\\frac{n_1}{\\delta x*N}\\) \\(\\frac{n1}{N}\\) \\(I_2\\) \\(x_2 -\\delta x/2, x_2+\\delta x/2\\) \\(n_2\\) \\(\\frac{n_2}{\\delta x*N}\\) \\(\\frac{n2}{N}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(I_k\\) \\(x_k -\\delta x/2, x_k+\\delta x/2\\) \\(n_k\\) \\(\\frac{n_k}{\\delta x*N}\\) \\(\\frac{nk}{N}\\) Total Figure 1.1: Histograms with different class intervals and a possible model for the pdf Then, for the \\(i^{th}\\) interval, \\[P(x_i - \\frac{\\delta x}{2} \\leq X \\leq x_i + \\frac{\\delta x}{2}) \\approx \\text{Area of the bar} \\] and therefore \\[\\text{Height of the bar} \\approx \\frac{\\text{Area of the bar}}{\\delta x} \\approx \\frac{ P(x_i - \\frac{\\delta x}{2} \\leq X \\leq x_i + \\frac{\\delta x}{2})}{\\delta x} \\] Therefore, the height of a bar represents the probability density in that class interval. When \\(\\delta x \\rightarrow0\\), it will allow us to approximate the histogram by a smooth curve as in Figure (d). As the are under each histogram is 1, the area under the curve is also 1 For any point \\(x\\), \\[\\text{The height of the curve} \\approx \\lim \\limits_{\\delta x \\to 0} \\frac{P(x_i-\\frac{\\delta x}{2} \\leq X \\leq x_i+\\frac{\\delta x}{2})}{\\delta x} \\] will represent the the probability density at point \\(x\\). Let the above smooth curve be denoted by \\(f_X\\). Then, \\(f_X\\) has the properties mentioned in Section . The function is called the probability density function of \\(X\\). NOTE Here \\(f_X(x)\\) represents Probability density at point \\(x\\). Not Probability at point \\(x\\). 1.3.3 Calculation of Probability using pdf Let \\(c,d \\in \\Re\\) such that \\(c\\leq d\\). Then, \\[P(c\\leq X \\leq d) = \\int_c^d f_X(x)dx\\] Figure 1.2: Histograms with different class intervals and a possible model for the pdf NOTE: if \\(X\\) is a continuous random variable with the p.d.f \\(f_X\\), then for any \\(k \\in \\Re\\), \\[P(X=k) = P(k\\leq X\\leq k)=\\int_k^kf_X(x)dx = 0\\] Therefore, for a continuous random variable \\(X\\), \\[P(c&lt;X&lt;d) = P(c\\leq X&lt;d) = P(c &lt; X\\leq d)= P(c \\leq X\\leq d)= \\int_c^df_X(x)dx\\] 1.4 Cumulative Distribution Function There are many problems in which it is of interest to know the probability that the values of a random variable is less than or equal to some real number \\(x\\). Definition 3: Cumulative distribution function The cumulative distibution function or \\(cdf\\) of a random variable \\(X\\), denoted by \\(F_X(x)\\), is defined by \\[F_x(x) = P(X\\leq x), \\text{ for all } x \\] Therefore, if \\(X\\) is a discrete random variable, the cdf is given by, \\[F_X(x)=\\sum_{t\\leq x}f_X(t), \\text{ } -\\infty &lt; x&lt;\\infty\\] where \\(f_X(t)\\) is the value of the pmf of \\(X\\) at \\(t\\). 1.4.1 Relationship between cdf and pdf If \\(X\\) is a continuous random variable, the cdf is given by, \\[F_X(x) = \\int_{-\\infty}^{x}f_X(t)dt \\text{ } -\\infty &lt;x&lt;\\infty\\] where \\(f_X(t)\\) is the value of the pdf of \\(X\\) at \\(t\\). (Here \\(t\\) is a dummy integration variable). Conversely, \\[f_X(x)= \\frac{dF_X(x)}{dx}\\] Example 06 An owner of a software engineering company is interested in knowing how many years his employees stay with his company. Let \\(X\\) be the number of years an employee will stay with the company. Over the years, he has established the following probability distribution: \\(x\\) 1 2 3 4 5 6 7 \\(f_X(x) = P(X=x)\\) 0.1 0.05 0.1 ? 0.3 0.2 0.1 Find \\(f_X(4)\\) Find \\(P(X&lt;4)\\) Find \\(P(X\\leq4)\\) Draw the probability mass function of \\(X\\) Draw the cumulative distribution function of \\(X\\) 1.4.2 Properties of a cumulative distribution function of a Discrete random variable 1.5 Descriptive properties of distributions 1.6 Models for discrete distributions 1.7 Models for continuous distributions "],
["estimations.html", "Chapter 2 Estimations 2.1 Point Estimation 2.2 Interval Estimation", " Chapter 2 Estimations 2.1 Point Estimation 2.1.1 Methods of finding point estimators 2.1.2 Methods of evaluating point estimators 2.2 Interval Estimation 2.2.1 Interpretation of confidence intervals 2.2.2 Methods of finding interval estimators 2.2.3 Methods of evaluating interval estimators "],
["hypothesis-testing.html", "Chapter 3 Hypothesis Testing 3.1 Null and alternative hypotheses 3.2 Errors in testing hypotheses-type I and type II error 3.3 Significance level, size, power of a test 3.4 Formulation of hypotheses 3.5 Methods of testing hypotheses", " Chapter 3 Hypothesis Testing 3.1 Null and alternative hypotheses 3.2 Errors in testing hypotheses-type I and type II error 3.3 Significance level, size, power of a test 3.4 Formulation of hypotheses 3.5 Methods of testing hypotheses "],
["design-of-experiments.html", "Chapter 4 Design of Experiments 4.1 ntroduction to experimental design 4.2 Basic principles of experimental design 4.3 Completely randomized design", " Chapter 4 Design of Experiments 4.1 ntroduction to experimental design 4.2 Basic principles of experimental design 4.3 Completely randomized design "],
["references.html", "References", " References "]
]
