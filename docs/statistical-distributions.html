<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Statistical Distributions | CM 2110 Calculus and Statistical Distributions</title>
  <meta name="description" content="Chapter 1 Statistical Distributions | CM 2110 Calculus and Statistical Distributions" />
  <meta name="generator" content="bookdown 0.18.3 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Statistical Distributions | CM 2110 Calculus and Statistical Distributions" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Statistical Distributions | CM 2110 Calculus and Statistical Distributions" />
  
  
  

<meta name="author" content="Dr.Â Priyanga D. Talagala" />


<meta name="date" content="2020-09-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="estimations.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">CM 1110 Fundamentals of Mathematics and Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course Syllabus</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#pre-requisites"><i class="fa fa-check"></i>Pre-requisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline-syllabus"><i class="fa fa-check"></i>Outline Syllabus</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#method-of-assessment"><i class="fa fa-check"></i>Method of Assessment</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recommended-texts"><i class="fa fa-check"></i>Recommended Texts</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lecturer"><i class="fa fa-check"></i>Lecturer</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="statistical-distributions.html"><a href="statistical-distributions.html"><i class="fa fa-check"></i><b>1</b> Statistical Distributions</a><ul>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#recap-cm-1110-probability"><i class="fa fa-check"></i>Recap: CM 1110-Probability</a><ul>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#axioms-of-probability"><i class="fa fa-check"></i>Axioms of probability</a></li>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#methods-for-determining-probability"><i class="fa fa-check"></i>Methods for determining Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#random-variable"><i class="fa fa-check"></i><b>1.1</b> Random Variable</a><ul>
<li class="chapter" data-level="1.1.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#types-of-random-variables"><i class="fa fa-check"></i><b>1.1.1</b> Types of Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>1.2</b> Probability Mass Function</a><ul>
<li class="chapter" data-level="1.2.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#properties-of-a-probability-mass-function"><i class="fa fa-check"></i><b>1.2.1</b> Properties of a Probability Mass Function</a></li>
<li class="chapter" data-level="1.2.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#representations-of-probability-mass-functions"><i class="fa fa-check"></i><b>1.2.2</b> Representations of Probability Mass Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>1.3</b> Probability Density Function</a><ul>
<li class="chapter" data-level="1.3.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#properties-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.3.1</b> Properties of a Probability Density Function</a></li>
<li class="chapter" data-level="1.3.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#existence-of-pdf"><i class="fa fa-check"></i><b>1.3.2</b> Existence of pdf</a></li>
<li class="chapter" data-level="1.3.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#calculation-of-probability-using-pdf"><i class="fa fa-check"></i><b>1.3.3</b> Calculation of Probability using pdf</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="statistical-distributions.html"><a href="statistical-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>1.4</b> Cumulative Distribution Function</a><ul>
<li class="chapter" data-level="1.4.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#relationship-between-cdf-and-pdf"><i class="fa fa-check"></i><b>1.4.1</b> Relationship between cdf and pdf</a></li>
<li class="chapter" data-level="1.4.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#properties-of-a-cumulative-distribution-function-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>1.4.2</b> Properties of a cumulative distribution function of a Discrete random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="statistical-distributions.html"><a href="statistical-distributions.html#expectations-and-moments"><i class="fa fa-check"></i><b>1.5</b> Expectations and Moments</a><ul>
<li class="chapter" data-level="1.5.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#expectation"><i class="fa fa-check"></i><b>1.5.1</b> Expectation</a></li>
<li class="chapter" data-level="1.5.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#moments"><i class="fa fa-check"></i><b>1.5.2</b> Moments</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="statistical-distributions.html"><a href="statistical-distributions.html#models-for-discrete-distributions"><i class="fa fa-check"></i><b>1.6</b> Models for Discrete Distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#discrete-uniform-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Discrete Uniform Distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#bernoulli-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Bernoulli Distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Binomial Distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="statistical-distributions.html"><a href="statistical-distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>1.6.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="1.6.5" data-path="statistical-distributions.html"><a href="statistical-distributions.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>1.6.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="1.6.6" data-path="statistical-distributions.html"><a href="statistical-distributions.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>1.6.6</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="1.6.7" data-path="statistical-distributions.html"><a href="statistical-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>1.6.7</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="statistical-distributions.html"><a href="statistical-distributions.html#models-for-continuous-distributions"><i class="fa fa-check"></i><b>1.7</b> Models for Continuous Distributions</a><ul>
<li class="chapter" data-level="1.7.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>1.7.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="1.7.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#normal-distribution-gaussian-distribution"><i class="fa fa-check"></i><b>1.7.2</b> Normal Distribution (Gaussian Distribution)</a></li>
<li class="chapter" data-level="1.7.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#gamma-distribution"><i class="fa fa-check"></i><b>1.7.3</b> Gamma Distribution</a></li>
<li class="chapter" data-level="1.7.4" data-path="statistical-distributions.html"><a href="statistical-distributions.html#exponential-distribution"><i class="fa fa-check"></i><b>1.7.4</b> Exponential Distribution</a></li>
<li class="chapter" data-level="1.7.5" data-path="statistical-distributions.html"><a href="statistical-distributions.html#beta-distribution"><i class="fa fa-check"></i><b>1.7.5</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="statistical-distributions.html"><a href="statistical-distributions.html#approximations"><i class="fa fa-check"></i><b>1.8</b> Approximations</a><ul>
<li class="chapter" data-level="1.8.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#poisson-approximation-to-binomial"><i class="fa fa-check"></i><b>1.8.1</b> Poisson approximation to Binomial</a></li>
<li class="chapter" data-level="1.8.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#normal-approximation-to-binomial"><i class="fa fa-check"></i><b>1.8.2</b> Normal approximation to Binomial</a></li>
<li class="chapter" data-level="1.8.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#normal-approximation-to-poisson"><i class="fa fa-check"></i><b>1.8.3</b> Normal approximation to Poisson</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="statistical-distributions.html"><a href="statistical-distributions.html#distribution-of-functions-of-random-variables"><i class="fa fa-check"></i><b>1.9</b> Distribution of Functions of Random Variables</a></li>
<li class="chapter" data-level="1.10" data-path="statistical-distributions.html"><a href="statistical-distributions.html#distribution-of-sum-of-independent-random-variables"><i class="fa fa-check"></i><b>1.10</b> Distribution of Sum of Independent Random Variables</a></li>
<li class="chapter" data-level="1.11" data-path="statistical-distributions.html"><a href="statistical-distributions.html#sampling-distribution"><i class="fa fa-check"></i><b>1.11</b> Sampling Distribution</a></li>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#references"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#tutorial"><i class="fa fa-check"></i>Tutorial</a></li>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="estimations.html"><a href="estimations.html"><i class="fa fa-check"></i><b>2</b> Estimations</a><ul>
<li class="chapter" data-level="2.1" data-path="estimations.html"><a href="estimations.html#statistical-inference"><i class="fa fa-check"></i><b>2.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="2.2" data-path="estimations.html"><a href="estimations.html#point-estimation"><i class="fa fa-check"></i><b>2.2</b> Point Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="estimations.html"><a href="estimations.html#methods-of-finding-point-estimators"><i class="fa fa-check"></i><b>2.2.1</b> Methods of finding point estimators</a></li>
<li class="chapter" data-level="2.2.2" data-path="estimations.html"><a href="estimations.html#desirable-properties-of-point-estimators"><i class="fa fa-check"></i><b>2.2.2</b> Desirable properties of point estimators</a></li>
<li class="chapter" data-level="2.2.3" data-path="estimations.html"><a href="estimations.html#consistency"><i class="fa fa-check"></i><b>2.2.3</b> Consistency</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="estimations.html"><a href="estimations.html#interval-estimation"><i class="fa fa-check"></i><b>2.3</b> Interval Estimation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="estimations.html"><a href="estimations.html#what-is-gained-by-using-an-interval-estimator"><i class="fa fa-check"></i><b>2.3.1</b> What is gained by using an Interval Estimator?</a></li>
<li class="chapter" data-level="2.3.2" data-path="estimations.html"><a href="estimations.html#definition-of-confidence-interval"><i class="fa fa-check"></i><b>2.3.2</b> Definition of confidence interval</a></li>
<li class="chapter" data-level="2.3.3" data-path="estimations.html"><a href="estimations.html#interpretation-of-confidence-intervals"><i class="fa fa-check"></i><b>2.3.3</b> Interpretation of confidence intervals</a></li>
<li class="chapter" data-level="2.3.4" data-path="estimations.html"><a href="estimations.html#methods-of-finding-interval-estimators"><i class="fa fa-check"></i><b>2.3.4</b> Methods of finding interval estimators</a></li>
<li class="chapter" data-level="2.3.5" data-path="estimations.html"><a href="estimations.html#methods-of-evaluating-interval-estimators"><i class="fa fa-check"></i><b>2.3.5</b> Methods of evaluating interval estimators</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="estimations.html"><a href="estimations.html#tutorial-1"><i class="fa fa-check"></i>Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>3</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>3.1</b> Null and alternative hypotheses</a></li>
<li class="chapter" data-level="3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#different-types-of-hypotheses"><i class="fa fa-check"></i><b>3.2</b> Different types of Hypotheses</a><ul>
<li class="chapter" data-level="3.2.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#writing-h_0-and-h_1"><i class="fa fa-check"></i><b>3.2.1</b> Writing <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span></a></li>
<li class="chapter" data-level="3.2.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#two-tailed-test-and-one-tailed-test"><i class="fa fa-check"></i><b>3.2.2</b> Two tailed test and one tailed test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#rejection-region"><i class="fa fa-check"></i><b>3.3</b> Rejection region</a></li>
<li class="chapter" data-level="3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#errors-in-testing-hypotheses-type-i-and-type-ii-error"><i class="fa fa-check"></i><b>3.4</b> Errors in testing hypotheses-type I and type II error</a></li>
<li class="chapter" data-level="3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#significance-level"><i class="fa fa-check"></i><b>3.5</b> Significance level</a></li>
<li class="chapter" data-level="3.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#power-of-a-test"><i class="fa fa-check"></i><b>3.6</b> Power of a test</a></li>
<li class="chapter" data-level="3.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#testing-hypotheses"><i class="fa fa-check"></i><b>3.7</b> Testing hypotheses</a></li>
<li class="chapter" data-level="3.8" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#methods-of-testing-hypotheses"><i class="fa fa-check"></i><b>3.8</b> Methods of testing hypotheses</a><ul>
<li class="chapter" data-level="3.8.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#using-the-critical-value-method-to-test-hypotheses"><i class="fa fa-check"></i><b>3.8.1</b> Using the critical value method to test hypotheses</a></li>
<li class="chapter" data-level="3.8.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#using-the-p-value-to-test-hypotheses"><i class="fa fa-check"></i><b>3.8.2</b> Using the p-value to test hypotheses</a></li>
<li class="chapter" data-level="3.8.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#using-confidence-intervals-to-test-hypotheses"><i class="fa fa-check"></i><b>3.8.3</b> Using confidence intervals to test hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#tutorial-2"><i class="fa fa-check"></i>Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="contingency-tables.html"><a href="contingency-tables.html"><i class="fa fa-check"></i><b>4</b> Contingency Tables</a></li>
<li class="chapter" data-level="5" data-path="design-of-experiments.html"><a href="design-of-experiments.html"><i class="fa fa-check"></i><b>5</b> Design of Experiments</a><ul>
<li class="chapter" data-level="5.1" data-path="design-of-experiments.html"><a href="design-of-experiments.html#introduction-to-experimental-design"><i class="fa fa-check"></i><b>5.1</b> Introduction to experimental design</a></li>
<li class="chapter" data-level="5.2" data-path="design-of-experiments.html"><a href="design-of-experiments.html#basic-principles-of-experimental-design"><i class="fa fa-check"></i><b>5.2</b> Basic principles of experimental design</a></li>
<li class="chapter" data-level="5.3" data-path="design-of-experiments.html"><a href="design-of-experiments.html#completely-randomized-design"><i class="fa fa-check"></i><b>5.3</b> Completely randomized design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CM 2110 Calculus and Statistical Distributions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-distributions" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Statistical Distributions</h1>

<div id="recap-cm-1110-probability" class="section level2 unnumbered">
<h2>Recap: CM 1110-Probability</h2>
<div id="axioms-of-probability" class="section level3 unnumbered">
<h3>Axioms of probability</h3>
<p><img src="figure/Axioms.png" width="100%" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Probability</strong> of an event quantifies the <strong>uncertainty</strong>, randomness, or the possibility of occurrence the event.</li>
<li>The probability of event E is usually denoted by <span class="math inline">\(P(E)\)</span>.</li>
<li>Mathematically, the function <span class="math inline">\(P(.)\)</span> is a set function defined from sample space <span class="math inline">\((\Omega)\)</span> to <span class="math inline">\([0, 1]\)</span> interval, satisfying the following properties.</li>
<li><p>These are called the <strong>âaxioms of probabilityâ</strong>.</p></li>
<li><strong>Axiom 1:</strong> For any event <span class="math inline">\(A\)</span>, <span class="math inline">\(P(A) \geq 0\)</span></li>
<li><strong>Axiom 2:</strong> <span class="math inline">\(P(\Omega) = 1\)</span></li>
<li><strong>Axiom 3:</strong>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(A_1, A_2, \dots, A_k\)</span> is a finite collection of mutually exclusive events, then <span class="math display">\[P(A_1\cup A_2\cup \dots \cup A_k)= \sum_{i=1}^kP(A_i)\]</span></li>
<li>If <span class="math inline">\(A_1, A_2, \dots\)</span> is an infinite collection of mutually exclusive events, then
<span class="math display">\[P(A_1\cup A_2\cup \dots)= \sum_{i=1}^\infty P(A_i)\]</span></li>
</ol></li>
</ul></li>
</ul>
<p><strong>NOTE</strong></p>
<ul>
<li>Axioms 1 and 2 imply that for any event <span class="math inline">\(E\)</span>, <span class="math inline">\(0 \leq P (E) \leq 1\)</span>.</li>
<li><span class="math inline">\(P (E) = 1 \iff\)</span> the event E is certain to occur.</li>
<li><span class="math inline">\(P (E) = 0 \iff\)</span> the event E cannot occur.</li>
</ul>
</div>
<div id="methods-for-determining-probability" class="section level3 unnumbered">
<h3>Methods for determining Probability</h3>
<ul>
<li>There are several ways for determining the probability of events.</li>
<li>Usually we use the following methods to obtain the probability of events.
<ul>
<li>Classical method</li>
<li>Relative frequency method (Empirical approach)</li>
<li>Subjective method</li>
<li><strong>Using probability models</strong></li>
</ul></li>
</ul>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="random-variable" class="section level2">
<h2><span class="header-section-number">1.1</span> Random Variable</h2>
<ul>
<li>Some sample spaces contain quantitative (numerical) outcomes, others contain qualitative outcomes.</li>
<li>Often it is convenient to work with sample spaces containing numerical outcomes.</li>
<li>A function that maps the original sample space into the real numbers is called a ârandom variableâ.</li>
<li>This is more useful when the original sample space contains qualitative outcomes.</li>
</ul>
<p><strong>Definition 1: Random Variable</strong></p>
<p>Let <span class="math inline">\(\Omega\)</span> be a sample space. Let <span class="math inline">\(X\)</span> be a function from <span class="math inline">\(\Omega\)</span> to <span class="math inline">\(\Re\)</span> (<em>i.e.</em> <span class="math inline">\(X:\Omega \rightarrow \Re\)</span>). Then <span class="math inline">\(X\)</span> is called a random variable.</p>
<!--**random variable** is a function from a sample space $S$ into the real numbers (*i.e.* $X:S \rightarrow \Re$)-->
<p><img src="figure/Ch1_F1.png" width="50%" style="display: block; margin: auto;" /></p>
<ul>
<li>A random variable assigns a real number to each outcome of a sample space.</li>
<li>In other words, to each outcome of an experiment or a sample point <span class="math inline">\(\omega_i\)</span>, of the sample spaces, there is a unique real number <span class="math inline">\(x_i\)</span>, known as the value of the random variable <span class="math inline">\(X\)</span>.</li>
<li>The range of the random variable is called the <em>induced sample space</em>.</li>
<li><em>A note on notation:</em> Random variables will always denoted with uppercase letters and the realized values of the random variable (or its range) will be denoted by the corresponding lowercase letters. Thus, the random variable <span class="math inline">\(X\)</span> can take the value <span class="math inline">\(x\)</span>.</li>
<li>Each outcome of a sample space occurs with a certain probability. Therefore, each possible value of a random variable is associated with a probability.</li>
<li>Any events of a sample space can be written in terms of a suitably defined random variable.</li>
</ul>
<div id="types-of-random-variables" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Types of Random Variables</h3>
<ul>
<li>A random variable is of two types
<ul>
<li>Discrete Random Variable</li>
<li>Continuous Random Variable</li>
</ul></li>
</ul>
<div id="discrete-random-variable" class="section level4">
<h4><span class="header-section-number">1.1.1.1</span> Discrete Random Variable</h4>
<ul>
<li>If the induced sample space is discrete, then the random variable is called a <strong>discrete random variable</strong>.
<!--- A random variable ($X$) is said to be discrete if it takes only a finite; or an infinite but countable number of values.-->
<!-- Examples: The following are discrete random variables:
  - Number of children per family 
  - Attendance of CM 2110 lectures
  - GPA credit value that you can obtain for CM 2110
  - The number of machine breakdowns during a given day in a company
--></li>
</ul>
<p><em>Example 01</em>
Consider the experiment of tossing a coin. Express the following events using a suitably defined random variable</p>
<p><span class="math inline">\(H=\)</span> <em>The event of getting a head</em></p>
<p><span class="math inline">\(T=\)</span> <em>The event of getting a tail</em></p>
<p><img src="figure/Ch1box1-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p><em>Example 02</em></p>
<p>Consider the experiment of rolling of a die. Express the following events using a suitably defined random variable</p>
<p><span class="math inline">\(A=\)</span> <em>The event that the number faced up is less than 5</em></p>
<p><span class="math inline">\(B=\)</span> <em>The event that the number faced up is even</em></p>
<p><span class="math inline">\(C=\)</span> <em>The event that the number faced up is 2 or 5</em></p>
<p><img src="figure/Ch1box2-1.png" width="100%" style="display: block; margin: auto;" /></p>
<div style="page-break-after: always;"></div>
<p><em>Example 03</em></p>
<p>Consider the experiment of tossing a coin 10 times. Then the sample space <span class="math inline">\(\Omega\)</span> contains <span class="math inline">\(2^{10} = 1024\)</span> outcomes. Each outcome is a sequence of 10 Hâs and Tâs.</p>
<p>Express the following events in terms of a suitably defined random variable.</p>
<p><span class="math inline">\(D=\)</span> <em>The event that the number of heads is 5</em></p>
<p><span class="math inline">\(E=\)</span> <em>The event that the number of tails is less than 4</em></p>
<p><img src="figure/Ch1box3-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="continuous-random-variable" class="section level4">
<h4><span class="header-section-number">1.1.1.2</span> Continuous Random Variable</h4>
<ul>
<li>If the induced sample space is continuous, then the random variable is called a <strong>continuous random variable.</strong>
<!--- A continuous random variable is a random variable that can take on any value in a given interval.
-   Random variables which consist of measurements are usually continuous. 
- For example
  - height of a student in this class 
  - the current measured in a thin copper wire in milliamperes
  - Life time of a mobile phone battery
  - SGPA of a level 2 student
--></li>
</ul>
<p><em>Example 04</em></p>
<p>Consider the experiment of measuring the lifetime (in hours) of a randomly selected bulb. Express the following events in terms of a suitably defined random variable.</p>
<p><span class="math inline">\(F=\)</span> <em>The event that the lifetime is less than 300 hours</em></p>
<p><span class="math inline">\(G=\)</span> <em>The event that the lifetime is 1000 hours</em></p>
<p><img src="figure/Ch1box4-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="probability-mass-function" class="section level2">
<h2><span class="header-section-number">1.2</span> Probability Mass Function</h2>
<p></p>
<p><strong>Definition 2: Discrete density function of a discrete random variable</strong></p>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable with distinct values <span class="math inline">\(x_1, x_2, \dots, x_n, \dots,\)</span> then the function, denoted by <span class="math inline">\(f_X(.)\)</span> and defined by</p>
<p><span class="math display">\[\begin{equation}
f_X(x) =
\begin{cases} 
P(X=x) &amp; \text{if } x=x_j, j=1,2,\dots,n,\dots\\
0 &amp; \text{if } x \neq x_j
\end{cases}
\end{equation}\]</span></p>
<p>is defined to be the discrete density function of <span class="math inline">\(X\)</span>.</p>
<!-- - The function that gives the probability of each possible value of a discrete random variable is called its probability mass function-->
<ul>
<li>The values of a discrete random variable are often called <em>mass points.</em></li>
<li><span class="math inline">\(f_X(x)\)</span> denotes the <em>mass</em> associated with the <em>mass point</em> <span class="math inline">\(x_j\)</span>.</li>
<li><strong><em>Probability mass function</em></strong> <em>discrete frequency function</em> and <em>probability function</em> are other terms used in place of <em>discrete density function</em></li>
<li>Probability function gives the measure of probability for different values of <span class="math inline">\(X\)</span>.</li>
</ul>
<div id="properties-of-a-probability-mass-function" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Properties of a Probability Mass Function</h3>
<ul>
<li>Let <span class="math inline">\(X\)</span> be a discrete random variable with probability mass function <span class="math inline">\(f_X(x)\)</span>. Then,</li>
</ul>
<ol style="list-style-type: decimal">
<li>For any <span class="math inline">\(x\in \Re\)</span>, <span class="math inline">\(0\leq f_X(x) \leq 1.\)</span></li>
<li>Let <span class="math inline">\(E\)</span> be an event and <span class="math inline">\(I= \{X(\omega):\omega \in E\}.\)</span> Then <span class="math inline">\(P(E) = P(X\in I) = \sum_{x \in I}f_X(x).\)</span></li>
<li>Let <span class="math inline">\(R = \{X(\omega):\omega \in \Omega\}.\)</span> Then <span class="math inline">\(\sum_{x\in \Re} f_X(x) = 1.\)</span></li>
</ol>
</div>
<div id="representations-of-probability-mass-functions" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Representations of Probability Mass Functions</h3>
<p><em>Example 05</em></p>
<p>Consider the experiment of tossing a fair coin. Let</p>
<p><span class="math display">\[\begin{equation}
X =
\begin{cases} 
0 &amp; \text{if the outcome is a Tail }\\
1 &amp; \text{if the outcome is a Head}
\end{cases}
\end{equation}\]</span></p>
<p>Find the probability mass function of <span class="math inline">\(X\)</span>. Is <span class="math inline">\(X\)</span> discrete or continuous?</p>
<div id="using-a-table" class="section level4">
<h4><span class="header-section-number">1.2.2.1</span> Using a table</h4>
<p><img src="figure/Ch1box5-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="using-a-function" class="section level4">
<h4><span class="header-section-number">1.2.2.2</span> Using a function</h4>
<p><img src="figure/Ch1box6-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="using-a-graph" class="section level4">
<h4><span class="header-section-number">1.2.2.3</span> Using a graph</h4>
<p><img src="figure/Ch1box7-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="probability-density-function" class="section level2">
<h2><span class="header-section-number">1.3</span> Probability Density Function</h2>
<ul>
<li><p>Let <span class="math inline">\(X\)</span> be a continuous random variable.</p></li>
<li><p>Then, it is not possible to define a pmf <span class="math inline">\(f_x\)</span> with properties mentioned in Section . <strong>Why?</strong></p></li>
<li><p>Instead, we can find a function <span class="math inline">\(f_x\)</span> with the some different properties.</p></li>
<li><p>Probability density function (pdf) of a continuous random variable is a function that describes the relative likelihood for this random variable to occur at a given point.</p></li>
</ul>
<div id="properties-of-a-probability-density-function" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Properties of a Probability Density Function</h3>
<p></p>
<p>Let <span class="math inline">\(X\)</span> be a continuous random variable with probability density function <span class="math inline">\(f_x\)</span>. Then,</p>
<ol style="list-style-type: decimal">
<li>For any <span class="math inline">\(x\in \Re\)</span>, <span class="math inline">\(f_X(x) \geq0\)</span>.</li>
<li>Let <span class="math inline">\(E\)</span> be an event and <span class="math inline">\(I= \{X(\omega):\omega \in E\}.\)</span> Then <span class="math inline">\(P(E) = P(X\in I) = \int_If_X(x)dx.\)</span></li>
<li>Let <span class="math inline">\(R = \{X(\omega):\omega \in \Omega\}.\)</span> Then <span class="math inline">\(\int_\Re f_X(x)dx= 1.\)</span></li>
</ol>
</div>
<div id="existence-of-pdf" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Existence of pdf</h3>
<ul>
<li>To see the existence of such a function, consider a continuous random variable <span class="math inline">\(X\)</span>,</li>
<li>Suppose that we have a very large number of observations, <span class="math inline">\(N\)</span>, of <span class="math inline">\(X\)</span>, measured to high accuracy (large number of decimal places).</li>
<li>consider the following grouped frequency table and the histogram constructed from those data.</li>
<li>The height of the bar on a class interval of this histogram is equal to the relative frequency per unit in that class interval.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Interval</th>
<th>Class boundaries</th>
<th>Class frequency</th>
<th>Height of the bar</th>
<th>Area of the bar</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(I_1\)</span></td>
<td><span class="math inline">\(x_1 -\delta x/2, x_1+\delta x/2\)</span></td>
<td><span class="math inline">\(n_1\)</span></td>
<td><span class="math inline">\(\frac{n_1}{\delta x*N}\)</span></td>
<td><span class="math inline">\(\frac{n1}{N}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(I_2\)</span></td>
<td><span class="math inline">\(x_2 -\delta x/2, x_2+\delta x/2\)</span></td>
<td><span class="math inline">\(n_2\)</span></td>
<td><span class="math inline">\(\frac{n_2}{\delta x*N}\)</span></td>
<td><span class="math inline">\(\frac{n2}{N}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(I_k\)</span></td>
<td><span class="math inline">\(x_k -\delta x/2, x_k+\delta x/2\)</span></td>
<td><span class="math inline">\(n_k\)</span></td>
<td><span class="math inline">\(\frac{n_k}{\delta x*N}\)</span></td>
<td><span class="math inline">\(\frac{nk}{N}\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:hist"></span>
<img src="figure/hist-1.png" alt="Histograms with different class intervals and a possible model for the pdf" width="2400" />
<p class="caption">
Figure 1.1: Histograms with different class intervals and a possible model for the pdf
</p>
</div>
<ul>
<li>Then, for the <span class="math inline">\(i^{th}\)</span> interval,</li>
</ul>
<p><span class="math display">\[P(x_i - \frac{\delta x}{2} \leq X \leq x_i + \frac{\delta x}{2}) \approx \text{Area of the bar} \]</span>
and therefore</p>
<p><span class="math display">\[\text{Height of the bar} \approx \frac{\text{Area of the bar}}{\delta x} \approx \frac{ P(x_i - \frac{\delta x}{2} \leq X \leq x_i + \frac{\delta x}{2})}{\delta x}  \]</span></p>
<ul>
<li><p>Therefore, the height of a bar represents the <em>probability density</em> in that class interval.</p></li>
<li><p>When <span class="math inline">\(\delta x \rightarrow0\)</span>, it will allow us to approximate the histogram by a smooth curve as in Figure  (d).</p></li>
<li><p>As the area under each histogram is 1, the area under the curve is also 1</p></li>
<li><p>For any point <span class="math inline">\(x\)</span>,</p></li>
</ul>
<p><span class="math display">\[\text{The height of the curve} \approx \lim \limits_{\delta x \to 0} \frac{P(x_i-\frac{\delta x}{2} \leq X \leq x_i+\frac{\delta x}{2})}{\delta x} \]</span></p>
<p>will represent the <strong>the probability density at point</strong> <span class="math inline">\(x\)</span>.</p>
<ul>
<li>Let the above smooth curve be denoted by <span class="math inline">\(f_X\)</span>.</li>
<li>Then, <span class="math inline">\(f_X\)</span> has the properties mentioned in Section .</li>
<li><p>The function is called the <strong>probability density function of</strong>
<span class="math inline">\(X\)</span>.</p></li>
<li><p><strong>NOTE</strong> Here <span class="math inline">\(f_X(x)\)</span> represents <strong>Probability density</strong> <em>at point</em> <span class="math inline">\(x\)</span>. Not <em>Probability at point</em> <span class="math inline">\(x\)</span>.</p></li>
</ul>
<!--The shape of the density function $f_x$ of a continuous random variable $X$ is approximately the same as the smoothed histogram from a very large data set $X$-->
</div>
<div id="calculation-of-probability-using-pdf" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Calculation of Probability using pdf</h3>
<ul>
<li>Let <span class="math inline">\(c,d \in \Re\)</span> such that <span class="math inline">\(c\leq d\)</span>. Then,</li>
</ul>
<p><span class="math display">\[P(c\leq X \leq d) = \int_c^d f_X(x)dx\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:hist2"></span>
<img src="figure/hist2-1.png" alt="$P(c\leq x \leq d = \int_c^d f_X(x)dx$" width="960" />
<p class="caption">
Figure 1.2: <span class="math inline">\(P(c\leq x \leq d = \int_c^d f_X(x)dx\)</span>
</p>
</div>
<ul>
<li><strong>NOTE:</strong> if <span class="math inline">\(X\)</span> is a continuous random variable with the p.d.f <span class="math inline">\(f_X\)</span>, then for any <span class="math inline">\(k \in \Re\)</span>,</li>
</ul>
<p><span class="math display">\[P(X=k) = P(k\leq X\leq k)=\int_k^kf_X(x)dx = 0\]</span></p>
<ul>
<li>Therefore, for a continuous random variable <span class="math inline">\(X\)</span>,
<span class="math display">\[P(c&lt;X&lt;d) = P(c\leq X&lt;d) =  P(c &lt; X\leq d)=  P(c \leq X\leq d)= \int_c^df_X(x)dx\]</span></li>
</ul>
</div>
</div>
<div id="cumulative-distribution-function" class="section level2">
<h2><span class="header-section-number">1.4</span> Cumulative Distribution Function</h2>
<ul>
<li>There are many problems in which it is of interest to know the probability that the values of a random variable is less than or equal to some real number <span class="math inline">\(x\)</span>.</li>
</ul>
<p><strong>Definition 3: Cumulative distribution function</strong></p>
<p>The <em>cumulative distibution function</em> or <span class="math inline">\(cdf\)</span> of a random variable <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(F_X(x)\)</span>, is defined by</p>
<p><span class="math display">\[F_x(x) = P(X\leq x), \text{ for all } x \]</span></p>
<ul>
<li>Therefore, if <span class="math inline">\(X\)</span> is a discrete random variable, the cdf is given by,</li>
</ul>
<p><span class="math display">\[F_X(x)=\sum_{t\leq x}f_X(t), \text{ } -\infty &lt; x&lt;\infty\]</span></p>
<p>where <span class="math inline">\(f_X(t)\)</span> is the value of the pmf of <span class="math inline">\(X\)</span> at <span class="math inline">\(t\)</span>.</p>
<div id="relationship-between-cdf-and-pdf" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Relationship between cdf and pdf</h3>
<ul>
<li>If <span class="math inline">\(X\)</span> is a continuous random variable, the cdf is given by,</li>
</ul>
<p><span class="math display">\[F_X(x) = \int_{-\infty}^{x}f_X(t)dt \text{  } -\infty &lt;x&lt;\infty\]</span>
where <span class="math inline">\(f_X(t)\)</span> is the value of the pdf of <span class="math inline">\(X\)</span> at <span class="math inline">\(t\)</span>. (Here <span class="math inline">\(t\)</span> is a dummy integration variable).</p>
<ul>
<li>Conversely,</li>
</ul>
<p><span class="math display">\[f_X(x)= \frac{dF_X(x)}{dx}\]</span></p>
<p><em>Example 06</em></p>
<p>An owner of a software engineering company is interested in knowing how many years his employees stay with his company. Let <span class="math inline">\(X\)</span> be the number of years an employee will stay with the company. Over the years, he has established the following probability distribution:</p>
<div style="page-break-after: always;"></div>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(f_X(x) = P(X=x)\)</span></td>
<td>0.1</td>
<td>0.05</td>
<td>0.1</td>
<td>?</td>
<td>0.3</td>
<td>0.2</td>
<td>0.1</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: decimal">
<li>Find <span class="math inline">\(f_X(4)\)</span></li>
</ol>
<p><img src="figure/Ch1box8-1.png" width="100%" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Find <span class="math inline">\(P(X&lt;4)\)</span></li>
</ol>
<p><img src="figure/Ch1box9-1.png" width="100%" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: decimal">
<li>Find <span class="math inline">\(P(X\leq4)\)</span></li>
</ol>
<p><img src="figure/Ch1box10-1.png" width="100%" style="display: block; margin: auto;" /></p>
<ol start="4" style="list-style-type: decimal">
<li>Draw the probability mass function of <span class="math inline">\(X\)</span></li>
</ol>
<p><img src="figure/Ch1box11-1.png" width="100%" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>Draw the cumulative distribution function of <span class="math inline">\(X\)</span></li>
</ol>
<p><img src="figure/Ch1box12-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p><img src="figure/Ch1box13-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="properties-of-a-cumulative-distribution-function-of-a-discrete-random-variable" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Properties of a cumulative distribution function of a Discrete random variable</h3>
<p><em>Example 07</em></p>
<p><span class="math display">\[\begin{equation}
f_X(x) =
\begin{cases} 
\frac{1}{25}x &amp; 0\leq x &lt; 5\\
\frac{2}{5}- \frac{1}{25}x &amp; 5\leq x \leq 10\\
0 &amp; \text{otherwise}
\end{cases}
\end{equation}\]</span></p>
<ol style="list-style-type: decimal">
<li>Find the CDF of <span class="math inline">\(X\)</span></li>
<li>Find <span class="math inline">\(P(X\leq 8)\)</span></li>
<li>Find <span class="math inline">\(P(3\leq X\leq 8)\)</span></li>
</ol>
<div style="page-break-after: always;"></div>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="expectations-and-moments" class="section level2">
<h2><span class="header-section-number">1.5</span> Expectations and Moments</h2>
<!-- Casella pg 55-->
<div id="expectation" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Expectation</h3>
<ul>
<li><p>The expected value, or expectation of a random variable is merely its average value.</p></li>
<li><p>By weighting the values of the random variable according to the probability distribution, we can obtain a number that summarizes a typical or expected value of an observation of the random variable.</p></li>
</ul>
<p><strong>Definition 4: Expected value</strong></p>
<p>Let <span class="math inline">\(X\)</span> be a random variable. The <em>expected value</em> or <em>mean</em> of a random variable <span class="math inline">\(g(X)\)</span>, denoted by <span class="math inline">\(E[g(x)],\)</span> is</p>
<p><span class="math display">\[\begin{equation}
E[g(x)] =
\begin{cases} 
\sum_{x}g(x) f_X(x) &amp; \text{if } X \text{ is a discrete random variable with pmf }  f_X(x) \\
\int_x g(x) f_X(x)dx &amp; \text{if } X \text{ is a continuous random variable with pdf }  f_X(x)
\end{cases}
\end{equation}\]</span></p>
<!--Casella pg 57-->
<ul>
<li><p>The mean of a random variable gives a measure of <em>central location</em> of the density of <span class="math inline">\(X\)</span>.</p></li>
<li><p>The process of taking expectations is a linear operation.</p></li>
<li><p>For any constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>,
<span class="math display">\[E(aX+b) = aE(X)+b\]</span></p></li>
</ul>
<div id="properties-of-expected-value" class="section level4">
<h4><span class="header-section-number">1.5.1.1</span> Properties of expected value</h4>
<!-- mood pg 70-, casella pg 57-->
<p><strong>Theorem</strong></p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(E(c) = c\)</span> for a constant <span class="math inline">\(c\)</span></li>
<li><span class="math inline">\(E[cg(X)] = cE[g(X)]\)</span> for a constant <span class="math inline">\(c\)</span></li>
<li><span class="math inline">\(E[c_1g_1(X)+c_2g_2(X)]= c_1E[g_1(X)]+c_2E[g_2(X)]\)</span></li>
<li>If <span class="math inline">\(g_1(x) \geq 0\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(E[g_1(X)] \geq 0\)</span></li>
<li>If <span class="math inline">\(g_1(x) \geq g_2(x)\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(E[g_1(X)] \geq E[g_2(X)]\)</span></li>
<li>If <span class="math inline">\(a\leq g_1(x) \leq b\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(a\leq E[g_1(X)] \leq b\)</span></li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two <strong>independent</strong> random variables, then <span class="math inline">\(E(X \times Y) = E(X)\times E(Y)\)</span></li>
</ol>
<p><em>Example 08</em></p>
<p>Random variable <span class="math inline">\(X\)</span> has the following pmf</p>
<p><span class="math display">\[\begin{equation}
f_X(x) =
\begin{cases} 
0.2 &amp;  x =2\\
0.3 &amp;  x =4\\
0.4 &amp;  x =5\\
0.1 &amp;  x =7
\end{cases}
\end{equation}\]</span></p>
<ol style="list-style-type: decimal">
<li>Find <span class="math inline">\(E(X)\)</span></li>
<li>Find <span class="math inline">\(E(X^2)\)</span></li>
<li>Find <span class="math inline">\(E\left( \frac{1}{X}\right)\)</span></li>
<li>Find <span class="math inline">\(E(2X+3X^2-5)\)</span></li>
</ol>
<p><img src="figure/Ch1box15-1.png" width="100%" style="display: block; margin: auto;" /></p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="moments" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Moments</h3>
<ul>
<li>The various moments of a distribution are an important class of expectation</li>
</ul>
<!-- Mood pg 73-->
<p><strong>Definition 5: Moments</strong></p>
<p>If <span class="math inline">\(X\)</span> is a random variable, the <span class="math inline">\(r\)</span><em>th moment of</em> <span class="math inline">\(X\)</span>, usually denoted by <span class="math inline">\(\mu_r^\prime,\)</span> is defined as</p>
<p><span class="math display">\[\mu_r^\prime = E(X^r).\]</span></p>
<p>if the expectation exists.</p>
<ul>
<li>Note that <span class="math inline">\(\mu_1^\prime = E(X)= \mu,\)</span> the mean of <span class="math inline">\(X\)</span>.</li>
</ul>
<p><strong>Definition 6: Central moments</strong></p>
<p>If <span class="math inline">\(X\)</span> is a random variable, the <span class="math inline">\(r\)</span><em>th central moment of</em> <span class="math inline">\(X\)</span> <em>about</em> <span class="math inline">\(a\)</span> is defined as <span class="math inline">\(E[(x-a)^r]\)</span>.</p>
<p>If <span class="math inline">\(a= E(X) = \mu,\)</span> we have the <span class="math inline">\(r\)</span><em>th central moment of</em> <span class="math inline">\(X\)</span>, <em>about</em> <span class="math inline">\(E(X)\)</span>, denoted by <span class="math inline">\(\mu_r,\)</span> which is
<span class="math display">\[\mu_r=E[(X-E(X))^r] = E[(X-\mu)^r] .\]</span></p>
<ul>
<li>Find <span class="math inline">\(\mu_1\)</span></li>
</ul>
<div style="page-break-after: always;"></div>
<p><strong>Definition 7: Variance</strong>
<!-- Casella pg 59-, mood 70--></p>
<p>If <span class="math inline">\(X\)</span> is a random variable, <span class="math inline">\(Var(X)= E[(X-E(X))^2]= E(X^2) - [E(X)]^2\)</span> provided <span class="math inline">\(E(X^2)\)</span> exists.</p>
<p><img src="figure/Ch1box16-1.png" width="100%" style="display: block; margin: auto;" /></p>
<ul>
<li><p>The <em>variance</em> of a random variable <span class="math inline">\(X\)</span> is its second central moment, <span class="math inline">\(Var(X) = E[(X-E(X))^2]= E[(X-\mu)^2]\)</span></p></li>
<li><p>The positive square root of <span class="math inline">\(Var(X)\)</span> is the <em>standard deviation</em> of <span class="math inline">\(X\)</span></p></li>
</ul>
<!-- mood 67-->
<ul>
<li><p>The <em>variance</em> of a random variable gives a measure of the degree of spread of a distribution around its mean.</p></li>
<li><p>Let <span class="math inline">\(X\)</span> be a random variable, and let <span class="math inline">\(\mu\)</span> be <span class="math inline">\(E(X).\)</span> the <em>variance</em> of <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(\sigma^2\)</span> of <span class="math inline">\(Var(X),\)</span> is defined by</p></li>
</ul>
<p><span class="math display">\[\begin{equation}
Var(X) =
\begin{cases} 
\sum_{x}(x-\mu)^2f_X(x) &amp; \text{if } X \text{ is  discrete with mass points }  x_!, x_2, \dots, x_j\dots \\
\int_x (x-\mu)^2 f_X(x)dx &amp; \text{if } X \text{ is continuous with probability density function }  f_X(x)
\end{cases}
\end{equation}\]</span></p>
<div id="properties-of-variance-of-a-random-variable" class="section level4">
<h4><span class="header-section-number">1.5.2.1</span> Properties of variance of a random variable</h4>
<!-- mood pg 70-, casella pg 57-->
<p><strong>Theorem</strong></p>
<ol style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(c\)</span> is a constant, then <span class="math inline">\(V(cX) = c^2V(X)\)</span></li>
<li><span class="math inline">\(V(c) = 0\)</span>, Variance of a constant is zero.</li>
<li>If <span class="math inline">\(X\)</span> is a random variable and <span class="math inline">\(c\)</span> is a constant, then <span class="math inline">\(V(c+X) = V(X)\)</span></li>
<li>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then <span class="math inline">\(V(aX+b)= a^2V(X)\)</span></li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two independent random variables, then
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(V(X+Y) = V(X) + V(Y)\)</span></li>
<li><span class="math inline">\(V(X-Y) = V(X) + V(Y)\)</span></li>
</ol></li>
</ol>
<p><img src="figure/Ch1box14-1.png" width="100%" style="display: block; margin: auto;" /></p>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
<div id="models-for-discrete-distributions" class="section level2">
<h2><span class="header-section-number">1.6</span> Models for Discrete Distributions</h2>
<div id="discrete-uniform-distribution" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Discrete Uniform Distribution</h3>
<!--Casella page 86-->
<p>A random variable <span class="math inline">\(X\)</span> has a <em>discrete uniform</em> <span class="math inline">\((1, N)\)</span> distribution if
<span class="math display">\[f_X(x)= P(X=x)=\frac{1}{N}, \;\; x=1,2,\dots, N\]</span>
where <span class="math inline">\(N\)</span> is a specified integer.</p>
<ul>
<li><p>The distribution puts equal mass on each of the outcomes <span class="math inline">\(1,2,\dots, N.\)</span></p></li>
<li><p>If <span class="math inline">\(X\)</span> ha s a discrete uniform distribution, then <span class="math inline">\(E(X) = (N+1)/2\)</span> and <span class="math inline">\(Var(X) = (N^2-1)/12.\)</span></p></li>
</ul>
</div>
<div id="bernoulli-distribution" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Bernoulli Distribution</h3>
<p><strong>Bernoulli Trial</strong></p>
<p>A random experiment of which the outcome can be classified into two categories is called a <em>Bernoulli trial</em></p>
<ul>
<li><p>In general, the results of a Bernoulli Trial are called âsuccessâ and âfailureâ. We denote these results by <span class="math inline">\(S\)</span> and <span class="math inline">\(F\)</span>, respectively.</p></li>
<li><p>Consider a Bernoulli trial. Let</p></li>
</ul>
<p><span class="math display">\[\begin{equation}
X=
\begin{cases} 
0 &amp; \text{if the Bernoulli trial results in a failure } \\
1 &amp; \text{if the Bernoulli trial results in a success} 
\end{cases}
\end{equation}\]</span></p>
<ul>
<li><p>Suppose that the probability of a ââsuccessâ in any Bernoulli trial is <span class="math inline">\(\theta.\)</span></p></li>
<li><p>Then <span class="math inline">\(X\)</span> is said to have a Bernoulli distribution with probability mass function</p></li>
</ul>
<p><span class="math inline">\(f_X(x)= P(X=x)=\theta^x(1-\theta)^{1-x}, \;\;x=0,1\)</span></p>
<ul>
<li><p>This is denoted as <span class="math inline">\(X \sim Bernoulli(\theta).\)</span></p></li>
<li><p>If <span class="math inline">\(X\)</span> has a Bernoulli distribution, then <span class="math inline">\(E(X) = \theta\)</span> and <span class="math inline">\(Var(X) = \theta(1-\theta)\)</span></p></li>
</ul>
</div>
<div id="binomial-distribution" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Binomial Distribution</h3>
<!--Mood page 89-->
<ul>
<li>A random experiment with the following properties is called a âBinomial experimentâ</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>The random experiment consists of a sequence of <span class="math inline">\(n\)</span> trials, where <span class="math inline">\(n\)</span> is fixed in advance of the random experiment.</p></li>
<li><p>Each trial can result in one of the same two possible outcomes: âsuccessâ <span class="math inline">\((S)\)</span> or âfailureâ <span class="math inline">\((F)\)</span></p></li>
<li><p>The trials are independent. Therefore the outcome of any particular trial does not influence the outcome of any other trial.</p></li>
<li><p>The probability of âsuccessâ is the same for each trial. Let this probability is <span class="math inline">\(\theta.\)</span></p></li>
</ol>
<p><strong>Binomial distribution</strong></p>
<ul>
<li>Consider a binomial experiment with <span class="math inline">\(n\)</span> trials and probability <span class="math inline">\(\theta\)</span> of a success.</li>
</ul>
<p>A random variable <span class="math inline">\(X\)</span> is defined to have a <em>binomial distribution</em> if the discrete density function of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[f_X(x)= P(X=x)= {n\choose x}\theta ^x(1-\theta)^{n-x} , \;\; x=0,1,2,\dots, n\]</span></p>
<ul>
<li><p>This is denoted as <span class="math inline">\(X \sim Bin(n,\theta).\)</span></p></li>
<li><p>If <span class="math inline">\(X\)</span> has a binomial distribution, then <span class="math inline">\(E(X) = n\theta\)</span> and <span class="math inline">\(Var(X) = n\theta(1-\theta)\)</span></p></li>
<li><p>The binomial distribution reduces to the Bernoulli distribution when <span class="math inline">\(n=1.\)</span></p></li>
</ul>
</div>
<div id="geometric-distribution" class="section level3">
<h3><span class="header-section-number">1.6.4</span> Geometric Distribution</h3>
<ul>
<li><p>Consider a sequence of independent Bernoulli trials whose probability of âsuccessâ for each trial is <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X= \text{Number of failures before the first success}\)</span></p></li>
<li><p>Then, <span class="math inline">\(X\)</span> is said to have a Geometric distribution with parameter <span class="math inline">\(\theta.\)</span></p></li>
<li><p>The probability mass function is given by
<span class="math display">\[f_X(x)= P(X=x)= \theta (1-\theta)^x , \;\; x=0,1,2,\dots\]</span></p></li>
<li><p>This is denoted as <span class="math inline">\(X \sim Geometric(\theta).\)</span></p></li>
<li><p>If <span class="math inline">\(X\)</span> has a geometric distribution, then <span class="math inline">\(E(X) = (1-\theta)/\theta\)</span> and <span class="math inline">\(Var(X) = (1-\theta)/\theta^2\)</span>
<!-- Mood page 101--></p></li>
<li><p>A random variable <span class="math inline">\(X\)</span> that has a geometric distribution is often referred to as a discrete <em>waiting-time</em> random variable. It represents how long (in terms of number of failures) one has to wait for a âsuccess.â</p></li>
</ul>
</div>
<div id="negative-binomial-distribution" class="section level3">
<h3><span class="header-section-number">1.6.5</span> Negative Binomial Distribution</h3>
<ul>
<li><p>Consider a sequence of independent Bernoulli trials whose probability of âsuccessâ for each trial is <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X= \text{Number of failures before the rth success}\)</span></p></li>
<li><p>Then, <span class="math inline">\(X\)</span> is said to have a Negative Binomial distribution with parameter <span class="math inline">\(\theta.\)</span></p></li>
<li><p>The probability mass function is given by
<span class="math display">\[f_X(x)= P(X=x)= {x+r-1\choose r-1} \theta^r (1-\theta)^x , \;\; x=0,1,2,\dots\]</span></p></li>
<li><p>This is denoted as <span class="math inline">\(X \sim negbin(r,\theta).\)</span></p></li>
<li><p>If <span class="math inline">\(X\)</span> has a Negative Binomial distribution, then <span class="math inline">\(E(X) = r(1-\theta)/\theta\)</span> and <span class="math inline">\(Var(X) = r(1-\theta)/\theta^2\)</span></p></li>
<li><p>If in the negative binomial distribution <span class="math inline">\(r=1\)</span>, then the negative binomial density specializes to the geometric density. <!-- mood page 99--></p></li>
</ul>
</div>
<div id="hypergeometric-distribution" class="section level3">
<h3><span class="header-section-number">1.6.6</span> Hypergeometric Distribution</h3>
<ul>
<li><p>Suppose a population of size <span class="math inline">\(N\)</span> has <span class="math inline">\(M\)</span> individuals of a certain kind (âsuccessâ).</p></li>
<li><p>A sample of <span class="math inline">\(n\)</span> items is taken from this population without replacement.</p></li>
<li><p>Let <span class="math inline">\(X\)</span> be the number of successes in the sample.</p></li>
<li><p>Then, <span class="math inline">\(X\)</span> is said to have a hypergeometric distribution.</p></li>
<li><p>The probability mass function is given by
<span class="math display">\[f_X(x)= P(X=x)= \frac{{M\choose x}{N-M\choose n-x}}{{N \choose n}}  , \;\; x=0,1,2,\dots, n\]</span></p></li>
<li><p>Hypergeometric distribution can be used as a model for the number of âsuccessesâ in a sample of size <span class="math inline">\(n\)</span> if the sampling is done without replacement from a relatively small population.</p></li>
<li><p>If <span class="math inline">\(X\)</span> has a Hypergeometric distribution, then <span class="math inline">\(E(X) = n.\frac{M}{N}\)</span> and <span class="math inline">\(Var(X) = n\frac{M}{N}\left(1-\frac{M}{N}\right)\left(\frac{N-n}{N-1}\right)\)</span></p></li>
</ul>
</div>
<div id="poisson-distribution" class="section level3">
<h3><span class="header-section-number">1.6.7</span> Poisson Distribution</h3>
<ul>
<li><p>The Poisson distribution provides a realistic probability model for the number of events in a given period of time, space, region or length.</p></li>
<li>Example:
<ul>
<li>The number of fatal traffic accidents per week in a given city</li>
<li>The number of emails per hour coming into the company of a large business</li>
<li>the number of defect per unit of some material</li>
</ul></li>
<li>Poisson distribution is suitable if the following conditions hold.
<ol style="list-style-type: decimal">
<li>the number of events within non-overlapping time intervals are independent.</li>
<li>Let <span class="math inline">\(t\)</span> be a fixed time point. For a small time interval <span class="math inline">\(\delta t\)</span>, the probability of exactly one event happening in the interval <span class="math inline">\([t,t+\delta t]\)</span> is approximately proportional to the length <span class="math inline">\(\delta t\)</span> of the interval. <em>i.e.,</em></li>
</ol></li>
</ul>
<p><span class="math display">\[\frac{P(\text{exactly one event in }[t,t+\delta t])}{\delta t} \to \text{a positive constant}\]</span>
as <span class="math inline">\(\delta t \to 0.\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Let <span class="math inline">\(t\)</span> be a fixed time point. For a small time interval <span class="math inline">\(\delta t\)</span>, the probability of more than one event happening in the interval <span class="math inline">\([t,t+\delta t]\)</span> is negligible. <em>i.e.,</em></li>
</ol>
<p><span class="math display">\[\frac{P(\text{more than one event in }[t,t+\delta t])}{\delta t} \to 0\]</span>
as <span class="math inline">\(\delta t \to 0.\)</span></p>
<ul>
<li>Let <span class="math inline">\(X\)</span> be the number of events during a time interval.</li>
<li>Suppose that the average number of events during the interested time interval is <span class="math inline">\(\lambda (&gt;0).\)</span></li>
<li>Then, the distribution of <span class="math inline">\(X\)</span> can be modeled by a Poisson Distribution with the probability mass function,</li>
</ul>
<p><span class="math display">\[f_X(x)= P(X=x)= \frac{e^{-\lambda} (\lambda)^x}{x!}  , \;\; x=0,1,2,\dots\]</span>
- This is denoted as <span class="math inline">\(X \sim Poisson(\lambda).\)</span></p>
<ul>
<li><p>If <span class="math inline">\(X\)</span> has a Poisson distribution, then <span class="math inline">\(E(X) = \lambda\)</span> and <span class="math inline">\(Var(X) = \lambda\)</span></p></li>
<li><p>The Poisson distribution can be used for counts of some sort of a given area, space, region, volume or length as well.</p></li>
</ul>
<p><em>Example 09</em></p>
<p>Phone calls arrive at a switchboard at an average rate of 2.0 calls per minute.</p>
<p>If the number of calls in any time interval follows the Poisson distribution, then</p>
<p><span class="math inline">\(X =\)</span> number of phone calls in a given minute.
<span class="math inline">\(X \sim Poisson(\; )\)</span></p>
<p><span class="math inline">\(Y =\)</span> number of phone calls in a given hour.
<span class="math inline">\(Y \sim Poisson ( \;)\)</span></p>
<p><span class="math inline">\(W =\)</span> number of phone calls in a 15 seconds.
<span class="math inline">\(W \sim Poisson (\;)\)</span></p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="models-for-continuous-distributions" class="section level2">
<h2><span class="header-section-number">1.7</span> Models for Continuous Distributions</h2>
<div id="uniform-distribution" class="section level3">
<h3><span class="header-section-number">1.7.1</span> Uniform Distribution</h3>
<ul>
<li><p>The continuous uniform distribution is defined by spreading mass uniformly over an interval <span class="math inline">\([a,b]\)</span>.</p></li>
<li><p>A random variable <span class="math inline">\(X\)</span> is said to have a uniform distribution in <span class="math inline">\((a,b)\)</span> if its probability density function is given by
<span class="math display">\[f_X(x) = \frac{1}{b-a};\; a\leq x\leq b\]</span></p></li>
<li><p>This is denoted as <span class="math inline">\(X\sim U(a,b)\)</span> or <span class="math inline">\(X \sim Unif(a, b)\)</span></p></li>
<li><p>It is easy to check <span class="math inline">\(\int_a^bf(x) dx =1\)</span>.</p></li>
<li><p>We also have
<span class="math display">\[E(X) = \int_a^b\frac{x}{b-a}dx = \frac{a+b}{2}\]</span></p></li>
</ul>
<p><span class="math display">\[Var(X) =  \frac{(b-a)^2}{12}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unif"></span>
<img src="figure/unif-1.png" alt="Uniform probabiity density" width="384" />
<p class="caption">
Figure 1.3: Uniform probabiity density
</p>
</div>
</div>
<div id="normal-distribution-gaussian-distribution" class="section level3">
<h3><span class="header-section-number">1.7.2</span> Normal Distribution (Gaussian Distribution)</h3>
<ul>
<li><p>One commonly used bell shaped curve is called the normal distribution.</p></li>
<li><p>Many techniques use din applied statistics are based upon the normal distribution.</p></li>
<li><p>The normal distribution has two parameters, usually denoted by <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, which are its mean and variance.</p></li>
<li><p>A random variable <span class="math inline">\(X\)</span> is said to have a normal distribution with location parameter <span class="math inline">\(\mu\)</span> and scale parameter <span class="math inline">\(\sigma\)</span>, if its probability density function is given by,</p></li>
</ul>
<p><span class="math display">\[f_X(x)=f_X(x;\mu, \sigma)= \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2};\;\; -\infty&lt;x&lt;\infty\]</span></p>
<ul>
<li><p>This is denoted by <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>.</p></li>
<li><p>The normal density function is <strong>symmetric around</strong> the location parameter <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>The <strong>dispersion of the distribution</strong> depends on the scale parameter <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>If <span class="math inline">\(X\)</span> is a normal random variable, <span class="math inline">\(E(X) = \mu\)</span> and <span class="math inline">\(Vax(X) = \sigma^2\)</span></p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:norm"></span>
<img src="figure/norm-1.png" alt="Normal distribution for different $\mu$ and $\sigma$" width="2400" />
<p class="caption">
Figure 1.4: Normal distribution for different <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:norm2"></span>
<img src="figure/norm2-1.png" alt="$P(a&lt; X &lt; b = \int_a^b f_X(x)dx$" width="960" />
<p class="caption">
Figure 1.5: <span class="math inline">\(P(a&lt; X &lt; b = \int_a^b f_X(x)dx\)</span>
</p>
</div>
<p><span class="math display">\[P(a&lt;X&lt;b)=\int_a^b \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}dx\]</span></p>
<ul>
<li>Evaluating of this integration is somewhat tedious</li>
<li>When we calculate this type of probabilities of normal distribution manually, it is convenient to use a normal probability table.</li>
</ul>
<div id="standard-normal-distribution" class="section level4">
<h4><span class="header-section-number">1.7.2.1</span> Standard Normal Distribution</h4>
<ul>
<li>Normal distribution with <span class="math inline">\(\mu =0\)</span> and <span class="math inline">\(\sigma = 1\)</span> is called the <strong>standard normal distribution</strong>.</li>
<li>A random variable with standard normal distribution is usually denoted by <span class="math inline">\(Z\)</span>.</li>
<li>The probability density function of standard normal distribution is denoted by <span class="math inline">\(\phi\)</span></li>
<li>If <span class="math inline">\(Z\sim N(0,1)\)</span>, then</li>
</ul>
<p><span class="math display">\[\phi_Z(z) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{1}{2}z^2};\;\; -\infty&lt; z&lt; \infty\]</span></p>
<ul>
<li>Probabilities related to Z can be found by using standard normal probability table.</li>
</ul>
<div style="page-break-after: always;"></div>
<p><em>Example 10</em></p>
<p>Let <span class="math inline">\(Z\)</span> be a standard normal random variable. Calculate probabilities given in table below.</p>
<table>
<thead>
<tr class="header">
<th>No.</th>
<th>Calculate this probability</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(P(Z &lt; 0)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(P(Z &lt; 2.02)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(P(Z &gt; 0.95)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td><span class="math inline">\(P(Z &gt; -1.48)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td><span class="math inline">\(P(Z &lt; -1.76)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td><span class="math inline">\(P(Z &lt; 1.7)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td><span class="math inline">\(P(Z &lt; -0.33)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>8</td>
<td><span class="math inline">\(P(0.94 &lt; Z &lt; 2.41)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>9</td>
<td><span class="math inline">\(P(-2.41 &lt; Z &lt; -0.94)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>10</td>
<td><span class="math inline">\(P(-2.96 &lt; Z &lt; 1.05)\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<div style="page-break-after: always;"></div>
<p>Example contdâ¦</p>
<div style="page-break-after: always;"></div>
<p><strong>Cumulative Standard Normal Distribution</strong></p>
<p><img src="figure/ztable.png" width="100%" style="display: block; margin: auto;" /></p>
<div style="page-break-after: always;"></div>
<p><span class="math inline">\(Z_\alpha\)</span> <strong>Notation</strong></p>
<ul>
<li><span class="math inline">\(Z_\alpha\)</span> denotes the value such that <span class="math inline">\(P(Z\geq Z_\alpha) = \alpha\)</span></li>
<li>Here <span class="math inline">\(\alpha\)</span> represents a probability.</li>
<li>Therfore <span class="math inline">\(0\leq \alpha \leq 1\)</span></li>
</ul>
<p><em>Example 11</em></p>
<p>Find <span class="math inline">\(Z_{0.025}\)</span>ââ</p>
<div class="figure" style="text-align: center"><span id="fig:norm3"></span>
<img src="figure/norm3-1.png" alt="$P(a&lt; X &lt; b = \int_a^b f_X(x)dx$" width="960" />
<p class="caption">
Figure 1.6: <span class="math inline">\(P(a&lt; X &lt; b = \int_a^b f_X(x)dx\)</span>
</p>
</div>
<p><em>Example 12</em></p>
<p>Find the following values</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(Z_{0.01}\)</span></li>
<li><span class="math inline">\(Z_{0.05}\)</span></li>
<li><span class="math inline">\(Z_{0.9}\)</span></li>
<li><span class="math inline">\(Z_{0.975}\)</span></li>
<li><span class="math inline">\(Z_{0.85}\)</span></li>
</ol>
<div style="page-break-after: always;"></div>
<p>Example contd â¦</p>
<div style="page-break-after: always;"></div>
</div>
<div id="calculation-of-probabilities-of-normal-distribution" class="section level4">
<h4><span class="header-section-number">1.7.2.2</span> Calculation of Probabilities of Normal Distribution</h4>
<ul>
<li>Suppose <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>.</li>
<li>Let <span class="math inline">\(Z = \frac{x-\mu}{\sigma}\)</span>.</li>
<li>Then, <span class="math inline">\(Z \sim N(0,1)\)</span></li>
<li>This result can be used to find probabilities of any normal distribution.</li>
</ul>
<p><em>Example 13</em></p>
<p>Let <span class="math inline">\(X \sim N(10,4).\)</span> Calculate <span class="math inline">\(P(X \geq 15)\)</span></p>
<p><img src="figure/Ch1box17-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p><em>Example 14</em></p>
<p>Calculate the probabilities in the table below</p>
<table>
<thead>
<tr class="header">
<th>No.</th>
<th><span class="math inline">\(\mu\)</span></th>
<th><span class="math inline">\(\sigma\)</span></th>
<th>Calculate this probability</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>95</td>
<td>16</td>
<td><span class="math inline">\(P(104.92 \leq X \leq 115.16)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td>65</td>
<td>15</td>
<td><span class="math inline">\(P(X \leq 86.66)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>96</td>
<td>20</td>
<td><span class="math inline">\(P( X &lt; 86.3)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>93</td>
<td>8</td>
<td><span class="math inline">\(P(91.24 \leq X \leq 109.34)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>63</td>
<td>9</td>
<td><span class="math inline">\(P(65.55 &lt; X &lt; 76.61)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>102</td>
<td>8</td>
<td><span class="math inline">\(P( X &gt; 80.55)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>79</td>
<td>18</td>
<td><span class="math inline">\(P(X &lt; 131.15)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>8</td>
<td>86</td>
<td>6</td>
<td><span class="math inline">\(P( X \leq 69.2)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>9</td>
<td>85</td>
<td>2</td>
<td><span class="math inline">\(P(X &lt; 86.46)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>10</td>
<td>100</td>
<td>5</td>
<td><span class="math inline">\(P(X \leq 112.26)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>11</td>
<td>58</td>
<td>10</td>
<td><span class="math inline">\(P(75.19 \leq X \leq 82.1)\)</span></td>
<td>0.0348</td>
</tr>
<tr class="even">
<td>12</td>
<td>49</td>
<td>7</td>
<td><span class="math inline">\(P(X \geq 48.52)\)</span></td>
<td>0.5273</td>
</tr>
<tr class="odd">
<td>13</td>
<td>103</td>
<td>17</td>
<td><span class="math inline">\(P(73.97 \leq X \leq 138.28)\)</span></td>
<td>0.9371</td>
</tr>
<tr class="even">
<td>14</td>
<td>99</td>
<td>24</td>
<td><span class="math inline">\(P( X &lt; 82.8)\)</span></td>
<td>0.2498</td>
</tr>
<tr class="odd">
<td>15</td>
<td>52</td>
<td>10</td>
<td><span class="math inline">\(P( X \leq 53.58)\)</span></td>
<td>0.5628</td>
</tr>
<tr class="even">
<td>16</td>
<td>72</td>
<td>8</td>
<td><span class="math inline">\(P(70.45 &lt; X \leq 93.5)\)</span></td>
<td>0.5732</td>
</tr>
<tr class="odd">
<td>17</td>
<td>82</td>
<td>20</td>
<td><span class="math inline">\(P(48.14 &lt; X &lt; 99.49)\)</span></td>
<td>0.7639</td>
</tr>
<tr class="even">
<td>18</td>
<td>94</td>
<td>15</td>
<td><span class="math inline">\(P(91.93 \leq X \leq 98.55)\)</span></td>
<td>0.1741</td>
</tr>
<tr class="odd">
<td>19</td>
<td>45</td>
<td>4</td>
<td><span class="math inline">\(P(42.36 \leq X \leq 50.59)\)</span></td>
<td>0.6643</td>
</tr>
<tr class="even">
<td>20</td>
<td>73</td>
<td>1</td>
<td><span class="math inline">\(P(X \geq 72.38)\)</span></td>
<td>0.7324</td>
</tr>
</tbody>
</table>
<p><em>Example 15</em></p>
<p>Calculate the quantiles <span class="math inline">\(k\)</span> in the table below</p>
<table>
<thead>
<tr class="header">
<th>No.</th>
<th><span class="math inline">\(\mu\)</span></th>
<th><span class="math inline">\(\sigma\)</span></th>
<th>Calculate <span class="math inline">\(k\)</span> such that</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>85</td>
<td>6</td>
<td><span class="math inline">\(P( X &lt;k)= 0.9936\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td>97</td>
<td>23</td>
<td><span class="math inline">\(P( X &lt;k)= 0.0694\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>77</td>
<td>5</td>
<td><span class="math inline">\(P( X &gt;k)= 0.0002\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>93</td>
<td>12</td>
<td><span class="math inline">\(P( X &gt;k)= 0.0023\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>67</td>
<td>3</td>
<td><span class="math inline">\(P( X &lt;k)= 0.0197\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>59</td>
<td>5</td>
<td><span class="math inline">\(P( X &gt;k)= 0.9756\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>94</td>
<td>13</td>
<td><span class="math inline">\(P( X &gt;k)= 0.3228\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>8</td>
<td>51</td>
<td>4</td>
<td><span class="math inline">\(P( X &lt;k)= 0.1515\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>9</td>
<td>49</td>
<td>10</td>
<td><span class="math inline">\(P( X &gt;k)= 0.9693\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>10</td>
<td>61</td>
<td>13</td>
<td><span class="math inline">\(P( X &lt;k)= 0.9946\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>11</td>
<td>69</td>
<td>14</td>
<td><span class="math inline">\(P( X &lt;k)= 0.9357\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>12</td>
<td>85</td>
<td>5</td>
<td><span class="math inline">\(P( X&gt;k)= 0.008\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>13</td>
<td>96</td>
<td>16</td>
<td><span class="math inline">\(P( X &gt;k)= 0.0014\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>14</td>
<td>96</td>
<td>7</td>
<td><span class="math inline">\(P( X &lt;k)= 0.2578\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>15</td>
<td>45</td>
<td>4</td>
<td><span class="math inline">\(P( X &lt;k)= 0.2578\)</span></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="empirical-rule-for-normal-distribution" class="section level4">
<h4><span class="header-section-number">1.7.2.3</span> Empirical Rule for Normal Distribution</h4>
<div class="figure" style="text-align: center"><span id="fig:norm4"></span>
<img src="figure/norm4-1.png" alt="Empirical Rule for Normal Distribution" width="960" />
<p class="caption">
Figure 1.7: Empirical Rule for Normal Distribution
</p>
</div>
<p>Calculate the following probabilities
1. <span class="math inline">\(P(|x-\mu|\leq \sigma)\)</span>
2. <span class="math inline">\(P(|x-\mu|\leq 2\sigma)\)</span>
3. <span class="math inline">\(P(|x-\mu|\leq 3\sigma)\)</span></p>
<p><strong>Empirical Rule for Normal Distribution</strong></p>
<p>Approximately 68% of the values in any normal distribution lie within one standard deviation, approximately 95% lie within two standard deviations and approximately 99.7% lie within three standard deviations from the mean.</p>
<ul>
<li><p>The normal distribution is somewhat special as its two parameters <span class="math inline">\(\mu\)</span> (the mean) and <span class="math inline">\(\sigma^2\)</span> (the variance), provide us with complete information about the exact shape and location of the distribution.</p></li>
<li><p>Straightforward calculus shows that the normal distribution has its maximum at <span class="math inline">\(x=\mu\)</span> and inflection point (where the curve changes from concave to convex) at <span class="math inline">\(\mu \pm\sigma.\)</span></p></li>
</ul>
</div>
</div>
<div id="gamma-distribution" class="section level3">
<h3><span class="header-section-number">1.7.3</span> Gamma Distribution</h3>
<ul>
<li><p>We come across with many practical situations in which the variable of interest has a skewed distribution.</p></li>
<li><p>The gamma family of distributions is a flexible family of distributions on <span class="math inline">\([0,\infty)\)</span> that yields a wide variety of skewed distributions</p></li>
</ul>
<!--casella-->
<p>A random variable <span class="math inline">\(X\)</span> is said to have a gamma distribution with shape parameter <span class="math inline">\(\alpha\)</span> and scale parameter <span class="math inline">\(\beta\)</span> if its probability density function is given by</p>
<p><span class="math display">\[f_X(x;\alpha, \beta)= \frac{1}{\Gamma(\alpha)\beta ^ {\alpha}}x^{\alpha-1}e^{-x/\beta}, \;\; 0&lt;x&lt;\infty,\;\; \alpha &gt;0, \;\;\beta &gt;0\]</span></p>
<p>Here <span class="math inline">\(\Gamma(\alpha)\)</span> is called the <em>gamma function</em>,</p>
<p><span class="math display">\[\Gamma(\alpha)=\int_0^{\infty}x^{\alpha-1}e^{-x}dx\]</span>
- The gamma function satisfies many useful relationships, in particular,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\Gamma(\alpha+1)=\alpha\Gamma(\alpha),\;\; \alpha &gt;0\)</span> (can be verified through integration by parts)</li>
<li><span class="math inline">\(\Gamma(1) =1\)</span></li>
<li>For any positive integer <span class="math inline">\(n(&gt;0)\)</span>, <span class="math inline">\(\Gamma(n)=(n-1)!\)</span></li>
<li><span class="math inline">\(\Gamma \left (\frac{1}{2} \right) = \sqrt{\pi}\)</span></li>
</ol>
<ul>
<li><p>When <span class="math inline">\(X\)</span> has a gamma distribution with shape parameter <span class="math inline">\(\alpha\)</span> and scale parameter <span class="math inline">\(\beta\)</span>, it is denoted as <span class="math inline">\(X \sim gamma(\alpha, \beta)\)</span></p></li>
<li><p>The parameter <span class="math inline">\(\alpha\)</span> is known as the shape parameter, since it most influences the peakedness of the distibution</p></li>
<li><p>The parameter <span class="math inline">\(\beta\)</span> is called the scale parameter, since most of its influence is on the spread of the distribution.</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:gamma"></span>
<img src="figure/gamma-1.png" alt="Gamma density functions" width="960" />
<p class="caption">
Figure 1.8: Gamma density functions
</p>
</div>
<ul>
<li><p>If <span class="math inline">\(X\)</span> has a gamma distribution with shape parameter <span class="math inline">\(\alpha\)</span> and scale parameter <span class="math inline">\(\beta\)</span>, then</p>
<ul>
<li><span class="math inline">\(E(X) = \alpha \beta\)</span></li>
<li><span class="math inline">\(Var(X) = \alpha \beta^2\)</span></li>
</ul></li>
</ul>
</div>
<div id="exponential-distribution" class="section level3">
<h3><span class="header-section-number">1.7.4</span> Exponential Distribution</h3>
<ul>
<li><p>This distribution is often used to model lifetime of various items.</p></li>
<li><p>When the number of events in a time interval has a Poisson distribution, the length of time interval between successive events can be modeled by an exponential distribution.</p></li>
<li><p>A random variable <span class="math inline">\(X\)</span> is said to have an exponential distribution with scale parameter <span class="math inline">\(\beta\)</span>, if its probability density function is given by</p></li>
</ul>
<p><span class="math display">\[f_X(x; \beta)=\frac{1}{\beta}e^{-x/\beta},\;\; 0&lt;x&lt;\infty\]</span></p>
<ul>
<li>This is denoted by <span class="math inline">\(X \sim exponential(\beta)\)</span></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:exp"></span>
<img src="figure/exp-1.png" alt="Exponential density functions" width="960" />
<p class="caption">
Figure 1.9: Exponential density functions
</p>
</div>
<ul>
<li><p>Note that exponential distibution is a special case of the gamma distribution.</p></li>
<li><p>It can be easily shown that <span class="math inline">\(X\sim exponential(\beta) \iff X\sim gamma (1.\beta)\)</span></p></li>
<li>If <span class="math inline">\(X\)</span> has an exponential distribution, then
<ul>
<li><span class="math inline">\(E(X) = \beta\)</span></li>
<li><span class="math inline">\(Var(X) = \beta^{2}\)</span></li>
</ul></li>
</ul>
</div>
<div id="beta-distribution" class="section level3">
<h3><span class="header-section-number">1.7.5</span> Beta Distribution</h3>
<ul>
<li><p>The beta family of distributions is a continuous family on <span class="math inline">\((0,1)\)</span> indexed by two parameters.</p></li>
<li><p>The <span class="math inline">\(beta(\alpha, \beta)\)</span> probability density function is</p></li>
</ul>
<p><span class="math display">\[f_X(x; \alpha, beta)= \frac{1}{B(\alpha, \beta)}x^{\alpha -1}(1-x)^{\beta-1},\;\;0&lt;x&lt;1,\; \alpha &gt;0,\;\; \beta &gt;0,\]</span></p>
<p>where <span class="math inline">\(B(\alpha, \beta)\)</span> denotes the <em>beta function</em>,
<span class="math display">\[B(\alpha, \beta)=\int_0^1x^{\alpha -1}(1-x)^{\beta-1}dx.\]</span></p>
<ul>
<li><p>The beta function is related to the gamma function through the following identity <span class="math display">\[B(\alpha, \beta)= \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\]</span></p></li>
<li><p>The beta distibution is one of the few common ânamedâ distributions that give probability 1 to a finite interval, here taken to be <span class="math inline">\((0,1)\)</span>.</p></li>
<li><p>Therefore, the beta distibution is often used to model proportions, which naturally lie between 0 and 1.</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:beta"></span>
<img src="figure/beta-1.png" alt="Beta density functions" width="960" />
<p class="caption">
Figure 1.10: Beta density functions
</p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="approximations" class="section level2">
<h2><span class="header-section-number">1.8</span> Approximations</h2>
<div id="poisson-approximation-to-binomial" class="section level3">
<h3><span class="header-section-number">1.8.1</span> Poisson approximation to Binomial</h3>
<p>Suppose <span class="math inline">\(X\sim Bin(n, \theta)\)</span> and <span class="math inline">\(n\)</span> is large and <span class="math inline">\(\theta\)</span> is small. Then <span class="math inline">\(X\sim Poisson(n\theta)\)</span> and</p>
<p><span class="math display">\[f_X(x) \approx \frac{e^{-n\theta} (n\theta)^x}{x!}\]</span></p>
<p><em>Example 16</em></p>
<p>Suppose <span class="math inline">\(X\)</span> has a binomial distribution with <span class="math inline">\(n=40\)</span> and <span class="math inline">\(p=0.005\)</span>. Find <span class="math inline">\(f_X(1)\)</span></p>
</div>
<div id="normal-approximation-to-binomial" class="section level3">
<h3><span class="header-section-number">1.8.2</span> Normal approximation to Binomial</h3>
<p>Suppose <span class="math inline">\(X\sim Bin(n, \theta)\)</span> and <span class="math inline">\(n\theta \geq 5\)</span> and <span class="math inline">\(n(1-\theta) \geq 5.\)</span> Then <span class="math inline">\(X \sim N(n\theta, n\theta(1-\theta))\)</span> and</p>
<p><span class="math display">\[P(X \leq x)_{Binomial} = P(X \leq x + 0.5)_{Normal} \approx P(Z \leq \frac{x+0.5-n\theta}{\sqrt{n\theta(1-\theta)}})\]</span></p>
<p><strong>Note:</strong> Since we are approximating a discrete distribution by a continuous distribution, we should apply a <em>continuity correction</em></p>
<p><em>Example 17</em></p>
<p>Suppose <span class="math inline">\(X\)</span> has a binomial distribution with <span class="math inline">\(n=40\)</span> and <span class="math inline">\(p=0.6.\)</span> Find</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P(X\leq 20)\)</span></li>
<li><span class="math inline">\(P(X &lt; 25)\)</span></li>
<li><span class="math inline">\(P(X &gt; 15)\)</span></li>
<li><span class="math inline">\(P(X \geq 20)\)</span></li>
<li><span class="math inline">\(P(X = 30)\)</span></li>
</ol>
</div>
<div id="normal-approximation-to-poisson" class="section level3">
<h3><span class="header-section-number">1.8.3</span> Normal approximation to Poisson</h3>
<p>Suppose <span class="math inline">\(X\sim Poisson(\lambda)\)</span> and <span class="math inline">\(\lambda &gt; 10,\)</span></p>
<p><span class="math display">\[P(X \leq x)_{Poisson} = P(X \leq x + 0.5)_{Normal} \approx P(Z \leq \frac{x+0.5-\lambda}{\sqrt{\lambda}})\]</span></p>
<p><strong>Note:</strong> Since we are approximating a discrete distribution by a continuous distribution, we should apply a <em>continuity correction</em></p>
<p><em>Example 18</em></p>
<p>Suppose <span class="math inline">\(X\)</span> has a Poisson distribution with <span class="math inline">\(\lambda=25\)</span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P(X\leq 20)\)</span></li>
<li><span class="math inline">\(P(X &lt; 25)\)</span></li>
<li><span class="math inline">\(P(X &gt; 15)\)</span></li>
<li><span class="math inline">\(P(X \geq 20)\)</span></li>
<li><span class="math inline">\(P(X = 30)\)</span></li>
</ol>
</div>
</div>
<div id="distribution-of-functions-of-random-variables" class="section level2">
<h2><span class="header-section-number">1.9</span> Distribution of Functions of Random Variables</h2>
<ol style="list-style-type: decimal">
<li><strong>Distribution of the linear transformation of a normal random variable</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2).\)</span> Let <span class="math inline">\(Y= ax+b,\)</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants. Then</p>
<p><span class="math display">\[Y\sim N(a\mu +b, a^2\sigma^2)\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Standardization of a normal random variable</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2).\)</span> Let <span class="math inline">\(Z= \frac{X-\mu}{\sigma}.\)</span> Then</p>
<p><span class="math display">\[Z\sim N(0,1).\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Distribution of the square of a standard normal random variable</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(Z \sim N(0,1).\)</span> Let <span class="math inline">\(Y= Z^2.\)</span> Then</p>
<p><span class="math display">\[Y\sim \chi^2_1\]</span></p>
<div style="page-break-after: always;"></div>
</div>
<div id="distribution-of-sum-of-independent-random-variables" class="section level2">
<h2><span class="header-section-number">1.10</span> Distribution of Sum of Independent Random Variables</h2>
<ol style="list-style-type: decimal">
<li><strong>Distribution of sum of i.i.d Bernoulli random variables</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent, identically distributed (i.i.d) random variables with <span class="math inline">\(Bernoulli(\theta)\)</span> distribution. Let <span class="math inline">\(Y= X_1+X_2+\dots +X_n.\)</span> Then</p>
<p><span class="math display">\[Y\sim Bin(n, \theta); \;\;y=0,1,2,\dots, n.\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Distribution of sum of i.i.d Poisson random variables</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent, identically distributed (i.i.d) random variables with <span class="math inline">\(Poisson(\lambda)\)</span> distribution. Let <span class="math inline">\(Y= X_1+X_2+\dots +X_n.\)</span> Then</p>
<p><span class="math display">\[Y\sim Poisson(n\lambda); \;\;y=0,1,2,\dots,.\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Distribution of sum of independent Poisson random variables</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent random variables with <span class="math inline">\(Poisson(\lambda_i), \; i=1,2,\dots, n.\)</span> Let <span class="math inline">\(Y= X_1+X_2+\dots +X_n.\)</span> Then</p>
<p><span class="math display">\[Y\sim Poisson \left(\sum_{i=1}^n\lambda_i\right); \;\;y=0,1,2,\dots,.\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Distribution of sum of i.i.d Geometric random variables</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent, identically distributed (i.i.d) random variables with <span class="math inline">\(Geometric(\theta)\)</span> distribution. Let <span class="math inline">\(Y= X_1+X_2+\dots +X_n.\)</span> Then</p>
<p><span class="math display">\[Y\sim Neg.bin(n, \theta); \;\;y=n, n+1, n+2,\dots,.\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Distribution of sum of independent Normal random variables</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent random variables with <span class="math inline">\(X_i\sim N(\mu_i, \sigma_i^2); \; i=1,2,\dots, n\)</span>. Let <span class="math inline">\(Y= X_1+X_2+\dots +X_n.\)</span> Then</p>
<p><span class="math display">\[Y\sim N\left(\sum_{i=1}^n \mu_i,\sum_{i=1}^n \sigma_i^2  \right).\]</span></p>
<ol start="6" style="list-style-type: decimal">
<li><strong>Distribution of sum of i.i.d exponential random variables</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent, identically distributed (i.i.d) random variables with <span class="math inline">\(X_i\sim exp(\lambda);\; i=1,2,\dots, n\)</span>. Let <span class="math inline">\(Y= X_1+X_2+\dots +X_n.\)</span> Then</p>
<p><span class="math display">\[Y \sim gamma(n, \lambda)\]</span></p>
<ol start="7" style="list-style-type: decimal">
<li><strong>Distribution of sum of independent gamma random variables</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent random variables with <span class="math inline">\(X_i\sim gamma(\alpha_i, \lambda);\; i=1,2,\dots, n\)</span>. Let <span class="math inline">\(Y= X_1+X_2+\dots +X_n.\)</span> Then</p>
<p><span class="math display">\[Y \sim gamma\left(\sum_{i=1}^n\alpha_i, \lambda\right).\]</span></p>
</div>
<div id="sampling-distribution" class="section level2">
<h2><span class="header-section-number">1.11</span> Sampling Distribution</h2>
<ol style="list-style-type: decimal">
<li><strong>Distribution of sample mean of a normal distribution</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(\bar{X} = \frac{X_1+X_2+\dots + X_n}{n}\)</span> be the sample mean. Then,</p>
<p><span class="math display">\[\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Distribution of sample variance of a normal distribution</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(\bar{X} = \frac{X_1+X_2+\dots + X_n}{n}\)</span> be the sample mean and <span class="math inline">\(S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2\)</span> be the sample variance. Then,</p>
<p><span class="math display">\[\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}.\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Large sample distribution of sample average - Central Limit Theorem)</strong></li>
</ol>
<p>Suppose that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> is a random sample from any distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(\bar{X} = \frac{X_1+X_2+\dots + X_n}{n}\)</span> be the sample mean and <span class="math inline">\(Z_n = \frac{\bar{X}_n-\mu}{\sigma/\sqrt{n}}.\)</span></p>
<p>Then, the distribution of <span class="math inline">\(Z_n\)</span> approaches the standard normal distribution as <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty.\)</span></p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<p>Casella, G., &amp; Berger, R. L. (2002). Statistical inference (Vol. 2, pp.Â 337-472).Pacific Grove, CA: Duxbury</p>
<p>Mood, A.M., Graybill, F.A. and Boes, D.C. (2007): Introduction to the Theory of Statistics, 3rd Edn. (Reprint). Tata McGraw-Hill Pub. Co.Â Ltd.</p>

</div>
<div id="tutorial" class="section level2 unnumbered">
<h2>Tutorial</h2>
<ol style="list-style-type: decimal">
<li>Consider the experiment of taking two products randomly form a production line and determine whether each is defective or not. Express the following events using a suitably defined random variable.</li>
</ol>
<p><span class="math inline">\(D_0=\)</span> <em>The event that both products are non defective</em></p>
<p><span class="math inline">\(D_1=\)</span> <em>The event that one product is defective</em></p>
<p><span class="math inline">\(D_2=\)</span> <em>The event that both products are defective</em></p>
<p><span class="math inline">\(E=\)</span> <em>The event that at least one product is defective</em></p>
<ol start="2" style="list-style-type: decimal">
<li>Let X be a random variable with the following probability distribution</li>
</ol>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th>1</th>
<th>1.5</th>
<th>2</th>
<th>2.5</th>
<th>3</th>
<th>other</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(f_X(x)\)</span></td>
<td><span class="math inline">\(k\)</span></td>
<td><span class="math inline">\(2k\)</span></td>
<td><span class="math inline">\(4k\)</span></td>
<td><span class="math inline">\(2k\)</span></td>
<td><span class="math inline">\(k\)</span></td>
<td>0</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>Find the value of <span class="math inline">\(k\)</span></li>
<li>Find <span class="math inline">\(P(X = 2.5)\)</span></li>
<li>Calculate <span class="math inline">\(P (X \geq 1.75)\)</span></li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>The sample space of a random experiment is <span class="math inline">\(\{a, b, c, d, e, f\}\)</span>, and each outcome is equally likely. A random variable <span class="math inline">\(X\)</span> is defined as follows:</li>
</ol>
<table>
<thead>
<tr class="header">
<th>Outcome</th>
<th><span class="math inline">\(a\)</span></th>
<th><span class="math inline">\(b\)</span></th>
<th><span class="math inline">\(c\)</span></th>
<th><span class="math inline">\(d\)</span></th>
<th><span class="math inline">\(e\)</span></th>
<th><span class="math inline">\(f\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x\)</span></td>
<td>0</td>
<td>0</td>
<td>1.5</td>
<td>1.5</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Determine the probability mass function of <span class="math inline">\(X\)</span>. Use the probability mass function to determine the following probabilities:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(P(X = 1.5)\)</span></li>
<li><span class="math inline">\(P(0.5 &lt; X &lt; 2.7)\)</span></li>
<li><span class="math inline">\(P(X &gt; 3)\)</span></li>
<li><span class="math inline">\(P(0 \leq X &lt; 2)\)</span></li>
<li><span class="math inline">\(P(X = 0 \text{ or } X = 2)\)</span></li>
</ol>
<ol start="4" style="list-style-type: decimal">
<li>Verify that the following function is a probability mass function, and determine the requested probabilities.</li>
</ol>
<p><span class="math display">\[f_X(x)=\frac{8}{7} \left(\frac{1}{2} \right)^x, \text{ x= 1,2,3 }\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(P(X \leq 1)\)</span></li>
<li><span class="math inline">\(P(X &gt; 1)\)</span></li>
<li><span class="math inline">\(P (2 &lt; X &lt; 6)\)</span></li>
<li><span class="math inline">\(P(X \leq 1 \text{ or } X &gt; 1)\)</span></li>
</ol>
<ol start="5" style="list-style-type: decimal">
<li>A disk drive manufacturer sells storage devices with capacities of one terabyte, 500 gigabytes, and 100 gigabytes with probabilities 0.5, 0.3, and 0.2, respectively. The revenues associated with the sales in that year are estimated to be $50 million, $25 million, and $10 million, respectively. Let <span class="math inline">\(X\)</span> denotes the revenue of storage devices during that year.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Determine the probability mass function of <span class="math inline">\(X\)</span>.</li>
<li>Calculate the probability of getting more than $20 million of revenue during that year.<br />
</li>
<li>Determine the cumulative distribution function of <span class="math inline">\(X\)</span></li>
</ol>
<ol start="6" style="list-style-type: decimal">
<li>Consider the following cumulative distribution function:</li>
</ol>
<p><span class="math display">\[\begin{equation}
F_X(x) =
\begin{cases} 
0 &amp; x&lt;-2\\
0.2 &amp; -2 \leq x&lt; 0\\
0.7 &amp; 0\leq x&lt;2\\
1 &amp; 2 \leq x
\end{cases}
\end{equation}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Draw the plot of <span class="math inline">\(F_X(x)\)</span></li>
<li>Discuss the properties of <span class="math inline">\(F_X(x)\)</span> (eg: whether it is discrete or continuous, whether it is decreasing or increasing function etc. )</li>
<li>Determine the probability mass function of <span class="math inline">\(X\)</span> from the above cumulative distribution function:</li>
<li>Plot the probability mass function of <span class="math inline">\(X\)</span></li>
</ol>
<ol start="7" style="list-style-type: decimal">
<li>The number of e-mail messages received per hour varies from 10 to 16 with the following probabilities</li>
</ol>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x =\)</span> number of messages</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P(X=x)\)</span></td>
<td>0.08</td>
<td>0.15</td>
<td>0.1</td>
<td>0.2</td>
<td>0.1</td>
<td>0.07</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(X\)</span> be the number of e-mail messages received per hour. Find the probability mass function of X</li>
<li>Determine the mean and standard deviation of the number of messages received per hour</li>
</ol>
<ol start="8" style="list-style-type: decimal">
<li>According to past data, twenty percent of all telephones of a certain type are submitted for service while under warranty. Of these, <span class="math inline">\(60\%\)</span> can be repaired whereas the other <span class="math inline">\(40\%\)</span> must be replaced with new units. If a company purchases ten of these telephones, what is the probability that</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>two telephones will be submitted for service under warranty?</li>
<li>at most 3 telephones will be submitted for service under warranty?</li>
<li>two telephones will end up being replaced under warranty?</li>
<li>one telephone will end up being repaired under warranty?</li>
</ol>
<ol start="9" style="list-style-type: decimal">
<li>Each sample of water has a <span class="math inline">\(10\%\)</span> chance of containing a particular organic pollutant. Assume that the samples are independent with regard to the presence of the pollutant. Find the probability that in the next 18 samples, exactly 2 contain the pollutant</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Define a suitable random variable for the above question</li>
<li>Find the distribution of that random variable</li>
<li>Find the probability that in the next 18 samples, exactly 2 contain the pollutant</li>
</ol>
<ol start="10" style="list-style-type: decimal">
<li>The space shuttle flight control system called Primary Avionics Software Set (PASS) uses four independent computers working in parallel. At each critical step, the computers âvoteâ to determine the appropriate step. The probability that a computer will ask for a roll to the left when a roll to the right is appropriate is 0.0001. Let <span class="math inline">\(X\)</span> denotes the number of computers that vote for a left roll when a right roll is appropriate.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>What is the probability mass function of <span class="math inline">\(X\)</span>?</li>
<li>What are the mean and variance of <span class="math inline">\(X\)</span>?</li>
</ol>
<ol start="11" style="list-style-type: decimal">
<li>A University lecturer never finishes his lecture before the end of the hour and always finishes his lectures within 2 minutes after the hour. Let <span class="math inline">\(X\)</span> = the time that elapses between the end of the hour and the end of the lecture and suppose the pdf of <span class="math inline">\(X\)</span> is</li>
</ol>
<p><span class="math display">\[\begin{equation}
f_X(x) =
\begin{cases} 
kx^2 &amp; 0\leq  x\leq 2\\
0 &amp; otherwise
\end{cases}
\end{equation}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Find the value of <span class="math inline">\(k\)</span> and draw the density curve.</li>
<li>What is the probability that the lecture ends within 1 min of the end of the hour?</li>
<li>What is the probability that the lecture continues beyond the hour for between 60 and 90 sec?</li>
<li>What is the probability that the lecture continues for at least 90 sec beyond the end of the hour?</li>
</ol>
<ol start="12" style="list-style-type: decimal">
<li>The daily sales of gasoline are uniformly distributed between 2,000 and 5,000 gallons. Find the probability that sales are:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>between 2,500 and 3,000 gallon</li>
<li>more than 4000 gallons</li>
<li>exactly 2500 gallons</li>
</ol>
<ol start="13" style="list-style-type: decimal">
<li>Suppose <span class="math inline">\(X\)</span> has a continuous uniform distribution over the interval <span class="math inline">\([-1,1]\)</span>. Determine the following:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Mean, variance, and standard deviation of <span class="math inline">\(X\)</span></li>
<li>Value for <span class="math inline">\(k\)</span> such that <span class="math inline">\(P(-k &lt; X &lt; k ) = 0.90\)</span></li>
<li>Cumulative distribution function</li>
</ol>
<ol start="14" style="list-style-type: decimal">
<li>Suppose that the time it takes a data collection operator to fill out an electronic form for a database is uniformly between 1.5 and 2.2 minutes.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>What are the mean and variance of the time it takes an operator to fill out the form?</li>
<li>What is the probability that it will take less than two minutes to fill out the form?</li>
<li>Determine the cumulative distribution function of the time it takes to fill out the form.</li>
</ol>
<ol start="15" style="list-style-type: decimal">
<li><p>An electronic product contains 40 integrated circuits. The probability that any integrated circuit is defective is 0.01, and the integrated circuits are independent. The product operates only if there are no defective integrated circuits. What is the probability that the product operates?</p></li>
<li><p>A bag of 200 chocolate chips is dumped into a batch of cookies dough. 40 cookies are made from such a batch of dough. What is the probability that a randomly selected cookie has at least 4 chocolate chips?</p></li>
<li><p>For the case of the thin copper wire, suppose that the number of flaws follows a Poisson distribution with a mean of 2.3 flaws per millimeter.</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Determine the probability of exactly two flaws in 1 millimeter of wire.</p></li>
<li><p>Determine the probability of 10 flaws in 5 millimeters of wire</p></li>
</ol>
<ol start="18" style="list-style-type: decimal">
<li>Contamination is a problem in the manufacture of magnetic storage disks. Assume that the number of particles of contamination that occur on a disk surface has a Poisson distribution, and the average number of particles per square centimeter of media surface is 0.1. The area of a disk under study is 100 square centimeters.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Determine the probability that 12 particles occur in the area of a disk under study.</p></li>
<li><p>Determine the probability that zero particles occur in the area of the disk under study.</p></li>
<li><p>Determine the probability that 12 or fewer particles occur in the area of the disk under study</p></li>
</ol>
<ol start="19" style="list-style-type: decimal">
<li>The number of surface flaws in plastic panels used in the interior of automobiles has a Poisson distribution with a mean of 0.05 flaw per square foot of plastic panel. Assume that an automobile interior contains 10 square feet of plastic panel.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>What is the probability that there are no surface flaws in an autoâs interior?</p></li>
<li><p>If 10 cars are sold to a rental company, what is the probability that none of the 10 cars has any surface flaws?</p></li>
<li><p>If 10 cars are sold to a rental company, what is the probability that at most 1 car has any surface flaws?</p></li>
</ol>
<ol start="20" style="list-style-type: decimal">
<li>Cabs pass your workplace according to a Poisson process with a mean of five cabs per hour. Suppose that you exit the workplace at 6:00 p.m. Determine the following:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Probability that you wait more than 10 minutes for a cab.</p></li>
<li><p>Probability that you wait fewer than 20 minutes for a cab.</p></li>
</ol>
<div style="page-break-after: always;"></div>
<ol start="11" style="list-style-type: decimal">
<li>An electronic product contains 40 integrated circuits. The probability that any integrated circuit is defective is 0.01, and the integrated circuits are independent. The product operates only if there are no defective integrated circuits. What is the probability that the product operates?</li>
</ol>
<!-- Activity 2.1.3 Thi note-->
<ol start="7" style="list-style-type: decimal">
<li>A person must take two buses to go to work. From the past experience, he knows that a bus can come at any time within 6 minutes. Also, the probability that a bus comes within any period of the same length is the same. Hence, it is reasonable to assume the following probability density function for the waiting time <span class="math inline">\(X\)</span> (in minutes) for a bus</li>
</ol>
<p><span class="math inline">\(f_X(x) = \frac{1}{6}, \;\;0&lt;x&lt;6.\)</span></p>
<p>Then the total waiting time <span class="math inline">\(T\)</span> at both bus-stops has the following density function</p>
<p><span class="math display">\[\begin{equation}
f_T(t) =
\begin{cases} 
\frac{t}{36} &amp; 0\leq t \leq 6\\
\frac{1}{3}- \frac{t}{36} &amp; 6\leq t \leq 12
\end{cases}
\end{equation}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Verify that each of above function is a proper density function</li>
<li>What is the probability that the waiting time at the first bus-stop will be less than 2 minutes?</li>
<li>What is the probability that the waiting time at the first bus-stop will be less than 2 minutes and the waiting time at the second</li>
</ol>

</div>
<div id="summary" class="section level2 unnumbered">
<h2>Summary</h2>
<p><strong>Models for Discrete Distributions</strong></p>
<table>
<colgroup>
<col width="11%" />
<col width="15%" />
<col width="19%" />
<col width="16%" />
<col width="19%" />
<col width="5%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Remarks</th>
<th>Probability mass function</th>
<th>values of <span class="math inline">\(X\)</span></th>
<th>Parameter Space</th>
<th>Mean</th>
<th>Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Discrete Uniform</td>
<td>Outcomes that are equally likely (finite)</td>
<td><span class="math inline">\(f(x) = \frac{1}{N}\)</span></td>
<td><span class="math inline">\(x=1,2,\dots, N\)</span></td>
<td><span class="math inline">\(N=1,2,\dots\)</span></td>
<td><span class="math inline">\(\frac{N+1}{2}\)</span></td>
<td><span class="math inline">\(\frac{N^2-1}{12}\)</span></td>
</tr>
<tr class="even">
<td>Bernoulli</td>
<td>Bernoulli trial</td>
<td><span class="math inline">\(f(x) = \theta^x (1-\theta)^{1-x}\)</span></td>
<td><span class="math inline">\(x=0,1\)</span></td>
<td><span class="math inline">\(0\leq \theta \leq 1\)</span></td>
<td><span class="math inline">\(\theta\)</span></td>
<td><span class="math inline">\(\theta (1-\theta)\)</span></td>
</tr>
<tr class="odd">
<td>Binomial</td>
<td><span class="math inline">\(X=\)</span> Number of successes in <span class="math inline">\(n\)</span> fixed trials</td>
<td><span class="math inline">\(f_X(x) = {n\choose x}\theta ^x(1-\theta)^{n-x}\)</span></td>
<td><span class="math inline">\(x=0,1,2,\dots, n\)</span></td>
<td><span class="math inline">\(0\leq \theta \leq 1; \;\;\; \newline n =1,2,3,\dots\)</span></td>
<td><span class="math inline">\(n\theta\)</span></td>
<td><span class="math inline">\(n\theta(1-\theta)\)</span></td>
</tr>
<tr class="even">
<td>Geometric</td>
<td><span class="math inline">\(X=\)</span> Number of failures before the first success</td>
<td><span class="math inline">\(f_X(x)= \theta (1-\theta)^x\)</span></td>
<td><span class="math inline">\(x=0,1,2,\dots\)</span></td>
<td><span class="math inline">\(0\leq \theta \leq 1\)</span></td>
<td><span class="math inline">\(\frac{1-\theta}{\theta}\)</span></td>
<td><span class="math inline">\(\frac{1-\theta}{\theta^2}\)</span></td>
</tr>
<tr class="odd">
<td>Negative Binomial</td>
<td><span class="math inline">\(X=\)</span> Number of failures before the <span class="math inline">\(r\)</span>th success</td>
<td><span class="math inline">\(f_X(x)= {x+r-1\choose r-1} \theta^r (1-\theta)^x\)</span></td>
<td><span class="math inline">\(x=0,1,2,\dots\)</span></td>
<td><span class="math inline">\(0\leq \theta \leq 1; \;\; r &gt; 0\)</span></td>
<td><span class="math inline">\(\frac{r(1-\theta)}{\theta}\)</span></td>
<td><span class="math inline">\(\frac{r(1-\theta)}{\theta^2}\)</span></td>
</tr>
<tr class="even">
<td>Hypergeometric</td>
<td><span class="math inline">\(X=\)</span> Number of successes in the sample taken without replacement</td>
<td><span class="math inline">\(f_X(x)= \frac{{M\choose x}{N-M\choose n-x}}{{N \choose n}}\)</span></td>
<td><span class="math inline">\(x=0,1,2,\dots, n\)</span></td>
<td><span class="math inline">\(N = 1,2,\dots;\;\;\;\;\newline M= 0,1,\dots,N; \;\;\;\; \newline n=1,2,\dots,N\)</span></td>
<td><span class="math inline">\(n\frac{M}{N}\)</span></td>
<td><span class="math inline">\(n\frac{M}{N}\left(1-\frac{M}{N} \right)\left(\frac{N-n}{N-1}\right)\)</span></td>
</tr>
<tr class="odd">
<td>Poisson</td>
<td><span class="math inline">\(X=\)</span> number of events in a given period of time, space, region or length</td>
<td><span class="math inline">\(f_X(x)= \frac{e^{-\lambda} (\lambda)^x}{x!}\)</span></td>
<td><span class="math inline">\(x=0,1,2,\dots\)</span></td>
<td><span class="math inline">\(\lambda &gt; 0\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Models for Continuous Distributions</strong></p>
<table>
<colgroup>
<col width="13%" />
<col width="27%" />
<col width="17%" />
<col width="22%" />
<col width="9%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Probability density function</th>
<th>Values of <span class="math inline">\(X\)</span></th>
<th>Parameter Space</th>
<th>Mean</th>
<th>Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Uniform</td>
<td><span class="math inline">\(f_X(x) = \frac{1}{b-a}\)</span></td>
<td><span class="math inline">\(a\leq x\leq b\)</span></td>
<td><span class="math inline">\(-\infty&lt;a&lt;b&lt;\infty\)</span></td>
<td><span class="math inline">\(\frac{a+b}{2}\)</span></td>
<td><span class="math inline">\(\frac{(b-a)^2}{12}\)</span></td>
</tr>
<tr class="even">
<td>Normal (Gaussian)</td>
<td><span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}\)</span></td>
<td><span class="math inline">\(-\infty&lt;x&lt;\infty\)</span></td>
<td><span class="math inline">\(-\infty&lt;\mu&lt;\infty;\;\; \sigma &gt;0\)</span></td>
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><span class="math inline">\(f_X(x)= \frac{1}{\Gamma(\alpha)\beta ^ {\alpha}}x^{\alpha-1}e^{-x/\beta}\)</span></td>
<td><span class="math inline">\(0&lt;x&lt;\infty\)</span></td>
<td><span class="math inline">\(\alpha &gt;0; \;\;\beta &gt;0\)</span></td>
<td><span class="math inline">\(\alpha\beta\)</span></td>
<td><span class="math inline">\(\alpha\beta^2\)</span></td>
</tr>
<tr class="even">
<td>Exponential</td>
<td><span class="math inline">\(f_X(x)=\frac{1}{\beta}e^{-x/\beta}\)</span></td>
<td><span class="math inline">\(0&lt;x&lt;\infty\)</span></td>
<td><span class="math inline">\(\beta &gt;0\)</span></td>
<td><span class="math inline">\(\beta\)</span></td>
<td><span class="math inline">\(\beta^2\)</span></td>
</tr>
<tr class="odd">
<td>Beta</td>
<td><span class="math inline">\(f_X(x)= \frac{1}{B(\alpha, \beta)}x^{\alpha -1}(1-x)^{\beta-1}\)</span></td>
<td><span class="math inline">\(0&lt;x&lt;1\)</span></td>
<td><span class="math inline">\(\alpha &gt;0;\;\; \beta &gt;0\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\alpha+\beta}\)</span></td>
<td><span class="math inline">\(\frac{\alpha\beta}{(\alpha+\beta+1)(\alpha+\beta)^2}\)</span></td>
</tr>
</tbody>
</table>


</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="estimations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-Statistical-Distorbutions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
